{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "df[df['TotalCharges'] == \" \"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (7043, 21)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n",
    "print(\"df shape: \", df.shape)\n",
    "\n",
    "cat_features = df.drop(['customerID','TotalCharges', 'MonthlyCharges', 'SeniorCitizen', 'tenure', 'Churn'],axis=1).columns\n",
    "cat_features\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ohe.fit(df[cat_features])\n",
    "\n",
    "dff = ohe.transform(df[cat_features])\n",
    "dff = pd.DataFrame(dff, columns=ohe.get_feature_names())\n",
    "dff = pd.concat([dff, df[['SeniorCitizen', 'MonthlyCharges', 'TotalCharges', 'tenure']]], axis=1)\n",
    "\n",
    "dff['TotalCharges'].astype('float')\n",
    "\n",
    "bin_dict = {'No':0, 'Yes':1}\n",
    "df.Churn = df.Churn.map(bin_dict)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_names = dff.columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = dff\n",
    "y = df.Churn\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7043, 21)\n",
      "\n",
      "\n",
      "X_train:  (5634, 45)\n",
      "y_train:  (5634,)\n",
      "X_test:  (1409, 45)\n",
      "y_test:  (1409,)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(\"\\n\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "#print(\"\\n\")\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperOpt Function for LightGBM, CatBoost and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import gc\n",
    "from hyperopt import hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "#optional but advised\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#GLOBAL HYPEROPT PARAMETERS\n",
    "NUM_EVALS = 1000 #number of hyperopt evaluation rounds\n",
    "N_FOLDS = 5 #number of cross-validation folds on data in each evaluation round\n",
    "\n",
    "#LIGHTGBM PARAMETERS\n",
    "LGBM_MAX_LEAVES = 2**11 #maximum number of leaves per tree for LightGBM\n",
    "LGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\n",
    "EVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \n",
    "EVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n",
    "\n",
    "#XGBOOST PARAMETERS\n",
    "XGB_MAX_LEAVES = 2**12 #maximum number of leaves when using histogram splitting\n",
    "XGB_MAX_DEPTH = 25 #maximum tree depth for XGBoost\n",
    "EVAL_METRIC_XGB_REG = 'mae' #XGBoost regression metric\n",
    "EVAL_METRIC_XGB_CLASS = 'auc' #XGBoost classification metric\n",
    "\n",
    "#CATBOOST PARAMETERS\n",
    "CB_MAX_DEPTH = 8 #maximum tree depth in CatBoost\n",
    "OBJECTIVE_CB_REG = 'MAE' #CatBoost regression metric\n",
    "OBJECTIVE_CB_CLASS = 'Logloss' #CatBoost classification metric\n",
    "\n",
    "#OPTIONAL OUTPUT\n",
    "BEST_SCORE = 0\n",
    "\n",
    "def quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False):\n",
    "    \n",
    "    #==========\n",
    "    #LightGBM\n",
    "    #==========\n",
    "    \n",
    "    if package=='lgbm':\n",
    "        \n",
    "        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth',\n",
    "                         'num_leaves',\n",
    "                          'max_bin',\n",
    "                         'min_data_in_leaf',\n",
    "                         'min_data_in_bin']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            #cast integer params from float to int\n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "            \n",
    "            #extract nested conditional parameters\n",
    "            if space_params['boosting']['boosting'] == 'goss':\n",
    "                top_rate = space_params['boosting'].get('top_rate')\n",
    "                other_rate = space_params['boosting'].get('other_rate')\n",
    "                #0 <= top_rate + other_rate <= 1\n",
    "                top_rate = max(top_rate, 0)\n",
    "                top_rate = min(top_rate, 0.5)\n",
    "                other_rate = max(other_rate, 0)\n",
    "                other_rate = min(other_rate, 0.5)\n",
    "                space_params['top_rate'] = top_rate\n",
    "                space_params['other_rate'] = other_rate\n",
    "            \n",
    "            subsample = space_params['boosting'].get('subsample', 1.0)\n",
    "            space_params['boosting'] = space_params['boosting']['boosting']\n",
    "            space_params['subsample'] = subsample\n",
    "            \n",
    "            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n",
    "            cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=True,\n",
    "                                early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_CLASS, seed=42)\n",
    "            \n",
    "            #best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            best_loss = 1 - cv_results['auc-mean'][-1]\n",
    "            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = lgb.Dataset(data, labels, free_raw_data = False)\n",
    "                \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = [{'boosting': 'gbdt',\n",
    "                          'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                         {'boosting': 'goss',\n",
    "                          'subsample': 1.0,\n",
    "                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n",
    "                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n",
    "        #metric_list = ['MAE', 'RMSE'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        metric_list = ['auc'] #modify as required for other classification metrics\n",
    "        objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n",
    "        objective_list_class = ['binary', 'cross_entropy']\n",
    "        #for classification set objective_list = objective_list_class\n",
    "        objective_list = objective_list_class\n",
    "\n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n",
    "                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n",
    "                'max_bin': hp.quniform('max_bin', 32, 255, 1),\n",
    "                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n",
    "                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n",
    "                'min_gain_to_split' : hp.quniform('min_gain_to_split', 0.1, 5, 0.01),\n",
    "                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n",
    "                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'metric' : hp.choice('metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n",
    "                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01),\n",
    "                'verbose': -1,\n",
    "                'n_jobs': 1\n",
    "            }\n",
    "        \n",
    "        #optional: activate GPU for LightGBM\n",
    "        #follow compilation steps here:\n",
    "        #https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm/\n",
    "        #then uncomment lines below:\n",
    "        #space['device'] = 'gpu'\n",
    "        #space['gpu_platform_id'] = 0,\n",
    "        #space['gpu_device_id'] =  0\n",
    "\n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "                \n",
    "        #fmin() will return the index of values chosen from the lists/arrays in 'space'\n",
    "        #to obtain actual values, index values are used to subset the original lists/arrays\n",
    "        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n",
    "        best['metric'] = metric_list[best['metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "                \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    #==========\n",
    "    #XGBoost\n",
    "    #==========\n",
    "    \n",
    "    if package=='xgb':\n",
    "        \n",
    "        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "                \n",
    "            #extract multiple nested tree_method conditional parameters\n",
    "            #libera te tutemet ex inferis\n",
    "            if space_params['tree_method']['tree_method'] == 'hist':\n",
    "                max_bin = space_params['tree_method'].get('max_bin')\n",
    "                space_params['max_bin'] = int(max_bin)\n",
    "                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n",
    "                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n",
    "                    space_params['grow_policy'] = grow_policy\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "                else:\n",
    "                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n",
    "                    space_params['grow_policy'] = 'lossguide'\n",
    "                    space_params['max_leaves'] = int(max_leaves)\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "            else:\n",
    "                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n",
    "                \n",
    "            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n",
    "            cv_results = xgb.cv(space_params, train, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_CLASS],\n",
    "                             early_stopping_rounds=100, stratified=True, seed=42, verbose_eval=0)\n",
    "            \n",
    "            #best_loss = cv_results['test-mae-mean'].iloc[-1] #or 'test-rmse-mean' if using RMSE\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n",
    "            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = xgb.DMatrix(data, labels)\n",
    "        \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n",
    "        #metric_list = ['MAE', 'RMSE'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        metric_list = ['auc']\n",
    "        #modify as required for other classification metrics classification\n",
    "        \n",
    "        tree_method = [{'tree_method' : 'exact'},\n",
    "               {'tree_method' : 'approx'},\n",
    "               {'tree_method' : 'hist',\n",
    "                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n",
    "                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n",
    "                                'grow_policy' : {'grow_policy':'lossguide',\n",
    "                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n",
    "        \n",
    "        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n",
    "        #'gpu_hist' in the nested dictionary above\n",
    "        \n",
    "        objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n",
    "        objective_list_class = ['reg:logistic', 'binary:logistic']\n",
    "        #for classification change line below to 'objective_list = objective_list_class'\n",
    "        objective_list = objective_list_class\n",
    "        \n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'tree_method' : hp.choice('tree_method', tree_method),\n",
    "                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n",
    "                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n",
    "                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n",
    "                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n",
    "                'gamma' : hp.uniform('gamma', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'eval_metric' : hp.choice('eval_metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n",
    "                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n",
    "                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n",
    "                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "                'nthread' : -1,\n",
    "                'verbosity': 0,\n",
    "            }\n",
    "        \n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "        \n",
    "        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n",
    "        best['boosting'] = boosting_list[best['boosting']]\n",
    "        best['eval_metric'] = metric_list[best['eval_metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "        \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        if 'max_leaves' in best:\n",
    "            best['max_leaves'] = int(best['max_leaves'])\n",
    "        if 'max_bin' in best:\n",
    "            best['max_bin'] = int(best['max_bin'])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        \n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    #==========\n",
    "    #CatBoost\n",
    "    #==========\n",
    "    \n",
    "    if package=='cb':\n",
    "        \n",
    "        print('Running {} rounds of CatBoost parameter optimisation:'.format(num_evals))\n",
    "        \n",
    "        #clear memory \n",
    "        gc.collect()\n",
    "            \n",
    "        integer_params = ['depth',\n",
    "                          #'one_hot_max_size', #for categorical data\n",
    "                          'min_data_in_leaf',\n",
    "                          'max_bin']\n",
    "        \n",
    "        def objective(space_params):\n",
    "                        \n",
    "            #cast integer params from float to int\n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "                \n",
    "            #extract nested conditional parameters\n",
    "            if space_params['bootstrap_type']['bootstrap_type'] == 'Bayesian':\n",
    "                bagging_temp = space_params['bootstrap_type'].get('bagging_temperature')\n",
    "                space_params['bagging_temperature'] = bagging_temp\n",
    "                \n",
    "            if space_params['grow_policy']['grow_policy'] == 'LossGuide':\n",
    "                max_leaves = space_params['grow_policy'].get('max_leaves')\n",
    "                space_params['max_leaves'] = int(max_leaves)\n",
    "                \n",
    "            space_params['bootstrap_type'] = space_params['bootstrap_type']['bootstrap_type']\n",
    "            space_params['grow_policy'] = space_params['grow_policy']['grow_policy']\n",
    "                           \n",
    "            #random_strength cannot be < 0\n",
    "            space_params['random_strength'] = max(space_params['random_strength'], 0)\n",
    "            #fold_len_multiplier cannot be < 1\n",
    "            space_params['fold_len_multiplier'] = max(space_params['fold_len_multiplier'], 1)\n",
    "                       \n",
    "            #for classification set stratified=True\n",
    "            cv_results = cb.cv(train, space_params, fold_count=N_FOLDS, \n",
    "                             early_stopping_rounds=25, stratified=True, partition_random_seed=42)\n",
    "           \n",
    "            #best_loss = cv_results['test-MAE-mean'].iloc[-1] #'test-RMSE-mean' for RMSE\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            best_loss = cv_results['test-Logloss-mean'].iloc[-1]\n",
    "            #if necessary, replace 'test-Logloss-mean' with 'test-[your-preferred-metric]-mean'\n",
    "            \n",
    "            return{'loss':best_loss, 'status': STATUS_OK}\n",
    "        \n",
    "        train = cb.Pool(data, labels.astype('float32'))\n",
    "        \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        bootstrap_type = [#{'bootstrap_type':'Poisson'}, \n",
    "                           {'bootstrap_type':'Bayesian',\n",
    "                            'bagging_temperature' : hp.loguniform('bagging_temperature', np.log(1), np.log(50))},\n",
    "                          {'bootstrap_type':'Bernoulli'}] \n",
    "        LEB = ['No', 'AnyImprovement'] #remove 'Armijo' if not using GPU\n",
    "        #score_function = ['Correlation', 'L2', 'NewtonCorrelation', 'NewtonL2']\n",
    "        grow_policy = [{'grow_policy':'SymmetricTree'},\n",
    "                       {'grow_policy':'Depthwise'},\n",
    "                       {'grow_policy':'Lossguide',\n",
    "                        'max_leaves': hp.quniform('max_leaves', 2, 32, 1)}]\n",
    "        eval_metric_list_reg = ['MAE', 'RMSE']\n",
    "        eval_metric_list_class = ['Logloss', 'AUC']\n",
    "        #for classification change line below to 'eval_metric_list = eval_metric_list_class'\n",
    "        eval_metric_list = eval_metric_list_class\n",
    "                \n",
    "        space ={'depth': hp.quniform('depth', 2, CB_MAX_DEPTH, 1),\n",
    "                'max_bin' : hp.quniform('max_bin', 1, 32, 1), #if using CPU just set this to 254\n",
    "                'l2_leaf_reg' : hp.uniform('l2_leaf_reg', 0, 5),\n",
    "                'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 1, 50, 1),\n",
    "                'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n",
    "                #'one_hot_max_size' : hp.quniform('one_hot_max_size', 2, 16, 1), #uncomment if using categorical features\n",
    "                'bootstrap_type' : hp.choice('bootstrap_type', bootstrap_type),\n",
    "                'learning_rate' : hp.uniform('learning_rate', 0.05, 0.25),\n",
    "                'eval_metric' : hp.choice('eval_metric', eval_metric_list),\n",
    "                'objective' : OBJECTIVE_CB_CLASS,\n",
    "                #'score_function' : hp.choice('score_function', score_function), #crashes kernel - reason unknown\n",
    "                'leaf_estimation_backtracking' : hp.choice('leaf_estimation_backtracking', LEB),\n",
    "                'grow_policy': hp.choice('grow_policy', grow_policy),\n",
    "                #'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),# CPU only\n",
    "                'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n",
    "                'od_type' : 'Iter',\n",
    "                'od_wait' : 25,\n",
    "                'task_type' : 'GPU',\n",
    "                'verbose' : 0,\n",
    "            }\n",
    "        \n",
    "        #optional: run CatBoost without GPU\n",
    "        #uncomment line below\n",
    "        space['task_type'] = 'CPU'\n",
    "            \n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "        \n",
    "        #unpack nested dicts first\n",
    "        best['bootstrap_type'] = bootstrap_type[best['bootstrap_type']]['bootstrap_type']\n",
    "        best['grow_policy'] = grow_policy[best['grow_policy']]['grow_policy']\n",
    "        best['eval_metric'] = eval_metric_list[best['eval_metric']]\n",
    "        \n",
    "        #best['score_function'] = score_function[best['score_function']] \n",
    "        #best['leaf_estimation_method'] = LEM[best['leaf_estimation_method']] #CPU only\n",
    "        best['leaf_estimation_backtracking'] = LEB[best['leaf_estimation_backtracking']]        \n",
    "        \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        if 'max_leaves' in best:\n",
    "            best['max_leaves'] = int(best['max_leaves'])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        \n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    else:\n",
    "        print('Package not recognised. Please use \"lgbm\" for LightGBM, \"xgb\" for XGBoost or \"cb\" for CatBoost.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1000 rounds of XGBoost parameter optimisation:\n",
      "100%|███████████████████████████████████████████| 1000/1000 [10:08<00:00,  1.64trial/s, best loss: 0.15438380000000007]\n",
      "{boosting: gblinear\n",
      "colsample_bylevel: 0.51\n",
      "colsample_bynode: 0.78\n",
      "colsample_bytree: 0.63\n",
      "eval_metric: auc\n",
      "gamma: 4.810365717428452\n",
      "learning_rate: 0.17634171799584678\n",
      "max_depth: 11\n",
      "min_child_weight: 4.235461704433606\n",
      "objective: reg:logistic\n",
      "reg_alpha: 0.006389914498800886\n",
      "reg_lambda: 2.1955950101749937\n",
      "subsample: 0.5\n",
      "tree_method: approx}\n"
     ]
    }
   ],
   "source": [
    "xgb_params = quick_hyperopt(X_train, y_train, 'xgb', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting': 'gblinear',\n",
       " 'colsample_bylevel': 0.51,\n",
       " 'colsample_bynode': 0.78,\n",
       " 'colsample_bytree': 0.63,\n",
       " 'eval_metric': 'auc',\n",
       " 'gamma': 4.810365717428452,\n",
       " 'learning_rate': 0.17634171799584678,\n",
       " 'max_depth': 11,\n",
       " 'min_child_weight': 4.235461704433606,\n",
       " 'objective': 'reg:logistic',\n",
       " 'reg_alpha': 0.006389914498800886,\n",
       " 'reg_lambda': 2.1955950101749937,\n",
       " 'subsample': 0.5,\n",
       " 'tree_method': 'approx'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'boosting': 'gblinear',\n",
    "    'colsample_bylevel': 0.51,\n",
    "    'colsample_bynode': 0.78,\n",
    "    'colsample_bytree': 0.63,\n",
    "    'eval_metric': 'auc',\n",
    "    'gamma': 4.810365717428452,\n",
    "    'learning_rate': 0.17634171799584678,\n",
    "    'max_depth': 11,\n",
    "    'min_child_weight': 4.235461704433606,\n",
    "    'objective': 'reg:logistic',\n",
    "    'reg_alpha': 0.006389914498800886,\n",
    "    'reg_lambda': 2.1955950101749937,\n",
    "    'subsample': 0.5,\n",
    "    'tree_method': 'approx'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:58:05] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { boosting } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.81563\tvalidation_1-auc:0.83390\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-auc:0.84048\tvalidation_1-auc:0.85461\n",
      "[2]\tvalidation_0-auc:0.84531\tvalidation_1-auc:0.85670\n",
      "[3]\tvalidation_0-auc:0.84714\tvalidation_1-auc:0.85668\n",
      "[4]\tvalidation_0-auc:0.84980\tvalidation_1-auc:0.85758\n",
      "[5]\tvalidation_0-auc:0.85298\tvalidation_1-auc:0.85917\n",
      "[6]\tvalidation_0-auc:0.85312\tvalidation_1-auc:0.85964\n",
      "[7]\tvalidation_0-auc:0.85593\tvalidation_1-auc:0.85991\n",
      "[8]\tvalidation_0-auc:0.85692\tvalidation_1-auc:0.86130\n",
      "[9]\tvalidation_0-auc:0.85690\tvalidation_1-auc:0.86221\n",
      "[10]\tvalidation_0-auc:0.85753\tvalidation_1-auc:0.86164\n",
      "[11]\tvalidation_0-auc:0.85868\tvalidation_1-auc:0.86162\n",
      "[12]\tvalidation_0-auc:0.85901\tvalidation_1-auc:0.86216\n",
      "[13]\tvalidation_0-auc:0.86080\tvalidation_1-auc:0.86187\n",
      "[14]\tvalidation_0-auc:0.86157\tvalidation_1-auc:0.86243\n",
      "[15]\tvalidation_0-auc:0.86327\tvalidation_1-auc:0.86276\n",
      "[16]\tvalidation_0-auc:0.86376\tvalidation_1-auc:0.86225\n",
      "[17]\tvalidation_0-auc:0.86440\tvalidation_1-auc:0.86213\n",
      "[18]\tvalidation_0-auc:0.86528\tvalidation_1-auc:0.86106\n",
      "[19]\tvalidation_0-auc:0.86597\tvalidation_1-auc:0.86155\n",
      "[20]\tvalidation_0-auc:0.86662\tvalidation_1-auc:0.86197\n",
      "Stopping. Best iteration:\n",
      "[15]\tvalidation_0-auc:0.86327\tvalidation_1-auc:0.86276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', boosting='gblinear',\n",
       "              colsample_bylevel=0.51, colsample_bynode=0.78,\n",
       "              colsample_bytree=0.63, eval_metric='auc', gamma=4.810365717428452,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.17634171799584678, max_delta_step=0, max_depth=11,\n",
       "              min_child_weight=4.235461704433606, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "              num_parallel_tree=1, objective='reg:logistic', random_state=0,\n",
       "              reg_alpha=0.006389914498800886, reg_lambda=2.1955950101749937,\n",
       "              scale_pos_weight=1, subsample=0.5, tree_method='approx',\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_xgb = xgb.XGBClassifier()\n",
    "clf_xgb.set_params(**xgb_params)\n",
    "clf_xgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7696973899109426"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_te = clf_xgb.predict(X_test)\n",
    "roc_auc_score(y_pred_te, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1000 rounds of LightGBM parameter optimisation:\n",
      "[LightGBM] [Warning]                                                                                                   \n",
      "bagging_fraction is set=0.81, subsample=1.0 will be ignored. Current value: bagging_fraction=0.81                      \n",
      "100%|███████████████████████████████████████████| 1000/1000 [11:45<00:00,  1.42trial/s, best loss: 0.15284779411501825]\n",
      "{bagging_fraction: 0.92\n",
      "boosting: goss\n",
      "feature_fraction: 0.59\n",
      "lambda_l1: 1.7651925160257511\n",
      "lambda_l2: 1.7825505465218088\n",
      "learning_rate: 0.16094579301919001\n",
      "max_bin: 184\n",
      "max_depth: 19\n",
      "metric: auc\n",
      "min_data_in_bin: 250\n",
      "min_data_in_leaf: 199\n",
      "min_gain_to_split: 2.04\n",
      "num_leaves: 1411\n",
      "objective: binary\n",
      "other_rate: 0.22505088017906216\n",
      "top_rate: 0.35438312429002566}\n"
     ]
    }
   ],
   "source": [
    "lgb_params = quick_hyperopt(X_train, y_train, 'lgbm', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'bagging_fraction': 0.92,\n",
    "    'boosting': 'goss',\n",
    "    'feature_fraction': 0.59,\n",
    "    'lambda_l1': 1.7651925160257511,\n",
    "    'lambda_l2': 1.7825505465218088,\n",
    "    'learning_rate': 0.16094579301919001,\n",
    "    'max_bin': 184,\n",
    "    'max_depth': 19,\n",
    "    'metric': 'auc',\n",
    "    'min_data_in_bin': 250,\n",
    "    'min_data_in_leaf': 199,\n",
    "    'min_gain_to_split': 2.04,\n",
    "    'num_leaves': 1411,\n",
    "    'objective': 'binary',\n",
    "    'other_rate': 0.22505088017906216,\n",
    "    'top_rate': 0.35438312429002566\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgb_params = {\n",
    "    'bagging_fraction': 0.71,\n",
    "    'boosting': 'goss',\n",
    "    'feature_fraction': 0.51,\n",
    "    'lambda_l1': 1.081403239398178,\n",
    "    'lambda_l2': 2.0663112415349176,\n",
    "    'learning_rate': 0.06809338955140036,\n",
    "    'max_bin': 50,\n",
    "    'max_depth': 18,\n",
    "    'metric': 'auc',\n",
    "    'min_data_in_bin': 195,\n",
    "    'min_data_in_leaf': 34,\n",
    "    'min_gain_to_split': 2.48,\n",
    "    'num_leaves': 1900,\n",
    "    'objective': 'cross_entropy',\n",
    "    'other_rate': 0.3489543869599704,\n",
    "    'top_rate': 0.040916784491823455,\n",
    "    'random_state': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=goss, boosting_type=gbdt will be ignored. Current value: boosting=goss\n",
      "[LightGBM] [Warning] feature_fraction is set=0.59, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.59\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=199, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=199\n",
      "[LightGBM] [Warning] min_gain_to_split is set=2.04, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=2.04\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.7651925160257511, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.7651925160257511\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.92, subsample=1.0 will be ignored. Current value: bagging_fraction=0.92\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.7825505465218088, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7825505465218088\n",
      "[1]\tvalid_0's auc: 0.841141\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's auc: 0.851137\n",
      "[3]\tvalid_0's auc: 0.852291\n",
      "[4]\tvalid_0's auc: 0.85224\n",
      "[5]\tvalid_0's auc: 0.854167\n",
      "[6]\tvalid_0's auc: 0.856377\n",
      "[7]\tvalid_0's auc: 0.8564\n",
      "[8]\tvalid_0's auc: 0.857155\n",
      "[9]\tvalid_0's auc: 0.857201\n",
      "[10]\tvalid_0's auc: 0.858432\n",
      "[11]\tvalid_0's auc: 0.859229\n",
      "[12]\tvalid_0's auc: 0.859815\n",
      "[13]\tvalid_0's auc: 0.859208\n",
      "[14]\tvalid_0's auc: 0.860169\n",
      "[15]\tvalid_0's auc: 0.860679\n",
      "[16]\tvalid_0's auc: 0.861034\n",
      "[17]\tvalid_0's auc: 0.861736\n",
      "[18]\tvalid_0's auc: 0.861634\n",
      "[19]\tvalid_0's auc: 0.861644\n",
      "[20]\tvalid_0's auc: 0.861754\n",
      "[21]\tvalid_0's auc: 0.861459\n",
      "[22]\tvalid_0's auc: 0.861904\n",
      "[23]\tvalid_0's auc: 0.861875\n",
      "[24]\tvalid_0's auc: 0.862095\n",
      "[25]\tvalid_0's auc: 0.862553\n",
      "[26]\tvalid_0's auc: 0.862524\n",
      "[27]\tvalid_0's auc: 0.86267\n",
      "[28]\tvalid_0's auc: 0.862876\n",
      "[29]\tvalid_0's auc: 0.862769\n",
      "[30]\tvalid_0's auc: 0.86281\n",
      "[31]\tvalid_0's auc: 0.862914\n",
      "[32]\tvalid_0's auc: 0.862903\n",
      "[33]\tvalid_0's auc: 0.863066\n",
      "[34]\tvalid_0's auc: 0.863139\n",
      "[35]\tvalid_0's auc: 0.863074\n",
      "[36]\tvalid_0's auc: 0.863113\n",
      "[37]\tvalid_0's auc: 0.863207\n",
      "[38]\tvalid_0's auc: 0.863148\n",
      "[39]\tvalid_0's auc: 0.863025\n",
      "[40]\tvalid_0's auc: 0.863197\n",
      "[41]\tvalid_0's auc: 0.863132\n",
      "[42]\tvalid_0's auc: 0.863106\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's auc: 0.863207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.92, boosting='goss', feature_fraction=0.59,\n",
       "               lambda_l1=1.7651925160257511, lambda_l2=1.7825505465218088,\n",
       "               learning_rate=0.16094579301919001, max_bin=184, max_depth=19,\n",
       "               metric='auc', min_data_in_bin=250, min_data_in_leaf=199,\n",
       "               min_gain_to_split=2.04, num_leaves=1411, objective='binary',\n",
       "               other_rate=0.22505088017906216, top_rate=0.35438312429002566)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lgb = lgb.LGBMClassifier()\n",
    "clf_lgb.set_params(**lgb_params) \n",
    "clf_lgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764436705876736"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_te = clf_lgb.predict(X_test)\n",
    "roc_auc_score(y_pred_te, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "epoch 0  | loss: 0.68866 | train_auc: 0.71802 | valid_auc: 0.6988  |  0:00:02s\n",
      "epoch 1  | loss: 0.53482 | train_auc: 0.75225 | valid_auc: 0.7553  |  0:00:04s\n",
      "epoch 2  | loss: 0.5015  | train_auc: 0.754   | valid_auc: 0.76153 |  0:00:06s\n",
      "epoch 3  | loss: 0.48533 | train_auc: 0.78012 | valid_auc: 0.81001 |  0:00:09s\n",
      "epoch 4  | loss: 0.47436 | train_auc: 0.80202 | valid_auc: 0.81279 |  0:00:12s\n",
      "epoch 5  | loss: 0.46137 | train_auc: 0.8167  | valid_auc: 0.8186  |  0:00:14s\n",
      "epoch 6  | loss: 0.45532 | train_auc: 0.81701 | valid_auc: 0.82122 |  0:00:16s\n",
      "epoch 7  | loss: 0.45082 | train_auc: 0.82065 | valid_auc: 0.82274 |  0:00:19s\n",
      "epoch 8  | loss: 0.4504  | train_auc: 0.8282  | valid_auc: 0.83147 |  0:00:21s\n",
      "epoch 9  | loss: 0.44048 | train_auc: 0.83433 | valid_auc: 0.83261 |  0:00:24s\n",
      "epoch 10 | loss: 0.43522 | train_auc: 0.8387  | valid_auc: 0.83114 |  0:00:27s\n",
      "epoch 11 | loss: 0.43191 | train_auc: 0.84145 | valid_auc: 0.83574 |  0:00:29s\n",
      "epoch 12 | loss: 0.43115 | train_auc: 0.8421  | valid_auc: 0.83633 |  0:00:32s\n",
      "epoch 13 | loss: 0.42705 | train_auc: 0.84921 | valid_auc: 0.83883 |  0:00:35s\n",
      "epoch 14 | loss: 0.4212  | train_auc: 0.85058 | valid_auc: 0.83591 |  0:00:37s\n",
      "epoch 15 | loss: 0.41249 | train_auc: 0.85642 | valid_auc: 0.83842 |  0:00:40s\n",
      "epoch 16 | loss: 0.41515 | train_auc: 0.85632 | valid_auc: 0.83719 |  0:00:42s\n",
      "epoch 17 | loss: 0.41076 | train_auc: 0.85756 | valid_auc: 0.8339  |  0:00:45s\n",
      "epoch 18 | loss: 0.40841 | train_auc: 0.85927 | valid_auc: 0.84174 |  0:00:48s\n",
      "epoch 19 | loss: 0.40882 | train_auc: 0.85978 | valid_auc: 0.83797 |  0:00:50s\n",
      "epoch 20 | loss: 0.40763 | train_auc: 0.86206 | valid_auc: 0.84026 |  0:00:53s\n",
      "epoch 21 | loss: 0.40713 | train_auc: 0.86319 | valid_auc: 0.83928 |  0:00:56s\n",
      "epoch 22 | loss: 0.4057  | train_auc: 0.86608 | valid_auc: 0.84118 |  0:00:58s\n",
      "epoch 23 | loss: 0.40482 | train_auc: 0.86816 | valid_auc: 0.83514 |  0:01:01s\n",
      "epoch 24 | loss: 0.40504 | train_auc: 0.86634 | valid_auc: 0.84408 |  0:01:03s\n",
      "epoch 25 | loss: 0.40526 | train_auc: 0.86999 | valid_auc: 0.83875 |  0:01:06s\n",
      "epoch 26 | loss: 0.40009 | train_auc: 0.86888 | valid_auc: 0.8402  |  0:01:08s\n",
      "epoch 27 | loss: 0.40455 | train_auc: 0.87196 | valid_auc: 0.84086 |  0:01:11s\n",
      "epoch 28 | loss: 0.39977 | train_auc: 0.87703 | valid_auc: 0.84386 |  0:01:14s\n",
      "epoch 29 | loss: 0.39202 | train_auc: 0.87879 | valid_auc: 0.84691 |  0:01:17s\n",
      "epoch 30 | loss: 0.38781 | train_auc: 0.88018 | valid_auc: 0.83911 |  0:01:19s\n",
      "epoch 31 | loss: 0.38719 | train_auc: 0.88018 | valid_auc: 0.84048 |  0:01:22s\n",
      "epoch 32 | loss: 0.38525 | train_auc: 0.88052 | valid_auc: 0.83737 |  0:01:25s\n",
      "epoch 33 | loss: 0.38562 | train_auc: 0.8825  | valid_auc: 0.841   |  0:01:27s\n",
      "epoch 34 | loss: 0.38256 | train_auc: 0.88714 | valid_auc: 0.84598 |  0:01:31s\n",
      "epoch 35 | loss: 0.37747 | train_auc: 0.88721 | valid_auc: 0.84098 |  0:01:34s\n",
      "epoch 36 | loss: 0.38154 | train_auc: 0.88232 | valid_auc: 0.83552 |  0:01:36s\n",
      "epoch 37 | loss: 0.3821  | train_auc: 0.88652 | valid_auc: 0.83597 |  0:01:39s\n",
      "epoch 38 | loss: 0.38363 | train_auc: 0.88879 | valid_auc: 0.83302 |  0:01:41s\n",
      "epoch 39 | loss: 0.37743 | train_auc: 0.88926 | valid_auc: 0.82956 |  0:01:43s\n",
      "\n",
      "Early stopping occured at epoch 39 with best_epoch = 29 and best_valid_auc = 0.84691\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "clf_tabnet = TabNetClassifier(n_d=32, n_a=32, seed=0)\n",
    "clf_tabnet.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['auc'],\n",
    "    max_epochs=1000 , \n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76954185520362"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_te = clf_tabnet.predict(X_test)\n",
    "roc_auc_score(y_pred_te, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_params = {\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'depth': 2,\n",
    "    'eval_metric': 'AUC',\n",
    "    'fold_len_multiplier': 1.8138033787766388,\n",
    "    'grow_policy': 'Lossguide',\n",
    "    'l2_leaf_reg': 2.6145900508841096,\n",
    "    'leaf_estimation_backtracking': 'AnyImprovement',\n",
    "    'learning_rate': 0.2211543774884907,\n",
    "    'max_bin': 32,\n",
    "    'max_leaves': 17,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'random_strength': 2.2878538537803936,\n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7878389\tbest: 0.7878389 (0)\ttotal: 4.2ms\tremaining: 4.2s\n",
      "1:\ttest: 0.8237718\tbest: 0.8237718 (1)\ttotal: 7.3ms\tremaining: 3.64s\n",
      "2:\ttest: 0.8202252\tbest: 0.8237718 (1)\ttotal: 10.3ms\tremaining: 3.44s\n",
      "3:\ttest: 0.8280469\tbest: 0.8280469 (3)\ttotal: 13.7ms\tremaining: 3.42s\n",
      "4:\ttest: 0.8387669\tbest: 0.8387669 (4)\ttotal: 17.1ms\tremaining: 3.4s\n",
      "5:\ttest: 0.8398990\tbest: 0.8398990 (5)\ttotal: 20.3ms\tremaining: 3.36s\n",
      "6:\ttest: 0.8414258\tbest: 0.8414258 (6)\ttotal: 23.7ms\tremaining: 3.36s\n",
      "7:\ttest: 0.8461175\tbest: 0.8461175 (7)\ttotal: 26.8ms\tremaining: 3.33s\n",
      "8:\ttest: 0.8489214\tbest: 0.8489214 (8)\ttotal: 30.1ms\tremaining: 3.31s\n",
      "9:\ttest: 0.8523684\tbest: 0.8523684 (9)\ttotal: 33.1ms\tremaining: 3.28s\n",
      "10:\ttest: 0.8535251\tbest: 0.8535251 (10)\ttotal: 36.1ms\tremaining: 3.25s\n",
      "11:\ttest: 0.8547440\tbest: 0.8547440 (11)\ttotal: 39.1ms\tremaining: 3.22s\n",
      "12:\ttest: 0.8539948\tbest: 0.8547440 (11)\ttotal: 42.1ms\tremaining: 3.19s\n",
      "13:\ttest: 0.8540168\tbest: 0.8547440 (11)\ttotal: 45.3ms\tremaining: 3.19s\n",
      "14:\ttest: 0.8540905\tbest: 0.8547440 (11)\ttotal: 48.4ms\tremaining: 3.17s\n",
      "15:\ttest: 0.8546857\tbest: 0.8547440 (11)\ttotal: 51.9ms\tremaining: 3.19s\n",
      "16:\ttest: 0.8551024\tbest: 0.8551024 (16)\ttotal: 54.9ms\tremaining: 3.17s\n",
      "17:\ttest: 0.8551878\tbest: 0.8551878 (17)\ttotal: 57.8ms\tremaining: 3.15s\n",
      "18:\ttest: 0.8554051\tbest: 0.8554051 (18)\ttotal: 61ms\tremaining: 3.15s\n",
      "19:\ttest: 0.8560987\tbest: 0.8560987 (19)\ttotal: 64.1ms\tremaining: 3.14s\n",
      "20:\ttest: 0.8560068\tbest: 0.8560987 (19)\ttotal: 67ms\tremaining: 3.12s\n",
      "21:\ttest: 0.8560974\tbest: 0.8560987 (19)\ttotal: 70ms\tremaining: 3.11s\n",
      "22:\ttest: 0.8560495\tbest: 0.8560987 (19)\ttotal: 73ms\tremaining: 3.1s\n",
      "23:\ttest: 0.8568026\tbest: 0.8568026 (23)\ttotal: 76.2ms\tremaining: 3.1s\n",
      "24:\ttest: 0.8572283\tbest: 0.8572283 (24)\ttotal: 79.2ms\tremaining: 3.09s\n",
      "25:\ttest: 0.8571907\tbest: 0.8572283 (24)\ttotal: 82.2ms\tremaining: 3.08s\n",
      "26:\ttest: 0.8571247\tbest: 0.8572283 (24)\ttotal: 85.5ms\tremaining: 3.08s\n",
      "27:\ttest: 0.8572464\tbest: 0.8572464 (27)\ttotal: 88.6ms\tremaining: 3.07s\n",
      "28:\ttest: 0.8574120\tbest: 0.8574120 (28)\ttotal: 91.6ms\tremaining: 3.07s\n",
      "29:\ttest: 0.8580693\tbest: 0.8580693 (29)\ttotal: 94.7ms\tremaining: 3.06s\n",
      "30:\ttest: 0.8579619\tbest: 0.8580693 (29)\ttotal: 97.6ms\tremaining: 3.05s\n",
      "31:\ttest: 0.8586153\tbest: 0.8586153 (31)\ttotal: 101ms\tremaining: 3.04s\n",
      "32:\ttest: 0.8590889\tbest: 0.8590889 (32)\ttotal: 103ms\tremaining: 3.03s\n",
      "33:\ttest: 0.8592040\tbest: 0.8592040 (33)\ttotal: 106ms\tremaining: 3.02s\n",
      "34:\ttest: 0.8594771\tbest: 0.8594771 (34)\ttotal: 109ms\tremaining: 3.01s\n",
      "35:\ttest: 0.8595379\tbest: 0.8595379 (35)\ttotal: 112ms\tremaining: 3s\n",
      "36:\ttest: 0.8596362\tbest: 0.8596362 (36)\ttotal: 115ms\tremaining: 3s\n",
      "37:\ttest: 0.8597307\tbest: 0.8597307 (37)\ttotal: 119ms\tremaining: 3s\n",
      "38:\ttest: 0.8594641\tbest: 0.8597307 (37)\ttotal: 121ms\tremaining: 2.99s\n",
      "39:\ttest: 0.8596737\tbest: 0.8597307 (37)\ttotal: 124ms\tremaining: 2.99s\n",
      "40:\ttest: 0.8596918\tbest: 0.8597307 (37)\ttotal: 127ms\tremaining: 2.98s\n",
      "41:\ttest: 0.8598186\tbest: 0.8598186 (41)\ttotal: 130ms\tremaining: 2.98s\n",
      "42:\ttest: 0.8602870\tbest: 0.8602870 (42)\ttotal: 133ms\tremaining: 2.97s\n",
      "43:\ttest: 0.8602172\tbest: 0.8602870 (42)\ttotal: 136ms\tremaining: 2.96s\n",
      "44:\ttest: 0.8601551\tbest: 0.8602870 (42)\ttotal: 139ms\tremaining: 2.95s\n",
      "45:\ttest: 0.8605406\tbest: 0.8605406 (45)\ttotal: 142ms\tremaining: 2.95s\n",
      "46:\ttest: 0.8603828\tbest: 0.8605406 (45)\ttotal: 145ms\tremaining: 2.95s\n",
      "47:\ttest: 0.8603155\tbest: 0.8605406 (45)\ttotal: 148ms\tremaining: 2.94s\n",
      "48:\ttest: 0.8602819\tbest: 0.8605406 (45)\ttotal: 151ms\tremaining: 2.93s\n",
      "49:\ttest: 0.8604682\tbest: 0.8605406 (45)\ttotal: 154ms\tremaining: 2.93s\n",
      "50:\ttest: 0.8605199\tbest: 0.8605406 (45)\ttotal: 157ms\tremaining: 2.93s\n",
      "Stopped by overfitting detector  (5 iterations wait)\n",
      "\n",
      "bestTest = 0.8605406441\n",
      "bestIteration = 45\n",
      "\n",
      "Shrink model to first 46 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x14eadeecbb0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cb = cb.CatBoostClassifier(**cb_params, early_stopping_rounds=5)\n",
    "clf_cb.fit(X_train, y_train, eval_set = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7704800681410423"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_te = clf_cb.predict(X_test)\n",
    "roc_auc_score(y_pred_te, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
