{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LOTAREA</th>\n",
       "      <th>BLDGTYPE</th>\n",
       "      <th>HOUSESTYLE</th>\n",
       "      <th>OVERALLCOND</th>\n",
       "      <th>YEARBUILT</th>\n",
       "      <th>ROOFSTYLE</th>\n",
       "      <th>EXTERCOND</th>\n",
       "      <th>FOUNDATION</th>\n",
       "      <th>BSMTCOND</th>\n",
       "      <th>...</th>\n",
       "      <th>GARAGETYPE</th>\n",
       "      <th>GARAGEFINISH</th>\n",
       "      <th>GARAGECARS</th>\n",
       "      <th>GARAGECOND</th>\n",
       "      <th>POOLAREA</th>\n",
       "      <th>POOLQC</th>\n",
       "      <th>FENCE</th>\n",
       "      <th>MOSOLD</th>\n",
       "      <th>YRSOLD</th>\n",
       "      <th>SALEPRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8450</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gable</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>TA</td>\n",
       "      <td>...</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>TA</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9600</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>Gable</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>TA</td>\n",
       "      <td>...</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>TA</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11250</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>Gable</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>TA</td>\n",
       "      <td>...</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>TA</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9550</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>Gable</td>\n",
       "      <td>TA</td>\n",
       "      <td>BrkTil</td>\n",
       "      <td>Gd</td>\n",
       "      <td>...</td>\n",
       "      <td>Detchd</td>\n",
       "      <td>Unf</td>\n",
       "      <td>3</td>\n",
       "      <td>TA</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>14260</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>Gable</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>TA</td>\n",
       "      <td>...</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>RFn</td>\n",
       "      <td>3</td>\n",
       "      <td>TA</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LOTAREA BLDGTYPE HOUSESTYLE  OVERALLCOND  YEARBUILT ROOFSTYLE  \\\n",
       "0   1     8450     1Fam     2Story            5       2003     Gable   \n",
       "1   2     9600     1Fam     1Story            8       1976     Gable   \n",
       "2   3    11250     1Fam     2Story            5       2001     Gable   \n",
       "3   4     9550     1Fam     2Story            5       1915     Gable   \n",
       "4   5    14260     1Fam     2Story            5       2000     Gable   \n",
       "\n",
       "  EXTERCOND FOUNDATION BSMTCOND  ... GARAGETYPE GARAGEFINISH GARAGECARS  \\\n",
       "0        TA      PConc       TA  ...     Attchd          RFn          2   \n",
       "1        TA     CBlock       TA  ...     Attchd          RFn          2   \n",
       "2        TA      PConc       TA  ...     Attchd          RFn          2   \n",
       "3        TA     BrkTil       Gd  ...     Detchd          Unf          3   \n",
       "4        TA      PConc       TA  ...     Attchd          RFn          3   \n",
       "\n",
       "  GARAGECOND  POOLAREA  POOLQC  FENCE  MOSOLD YRSOLD  SALEPRICE  \n",
       "0         TA         0     NaN    NaN       2   2008     208500  \n",
       "1         TA         0     NaN    NaN       5   2007     181500  \n",
       "2         TA         0     NaN    NaN       9   2008     223500  \n",
       "3         TA         0     NaN    NaN       2   2006     140000  \n",
       "4         TA         0     NaN    NaN      12   2008     250000  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('predict_home_value.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['BLDGTYPE','HOUSESTYLE','ROOFSTYLE','EXTERCOND','FOUNDATION','BSMTCOND','HEATING','HEATINGQC','CENTRALAIR','ELECTRICAL','KITCHENQUAL','FIREPLACEQU','GARAGETYPE','GARAGEFINISH','GARAGECOND','POOLQC','FENCE']\n",
    "num_cols = df.select_dtypes(include=['int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLDGTYPE</th>\n",
       "      <th>HOUSESTYLE</th>\n",
       "      <th>ROOFSTYLE</th>\n",
       "      <th>EXTERCOND</th>\n",
       "      <th>FOUNDATION</th>\n",
       "      <th>BSMTCOND</th>\n",
       "      <th>HEATING</th>\n",
       "      <th>HEATINGQC</th>\n",
       "      <th>CENTRALAIR</th>\n",
       "      <th>ELECTRICAL</th>\n",
       "      <th>KITCHENQUAL</th>\n",
       "      <th>FIREPLACEQU</th>\n",
       "      <th>GARAGETYPE</th>\n",
       "      <th>GARAGEFINISH</th>\n",
       "      <th>GARAGECOND</th>\n",
       "      <th>POOLQC</th>\n",
       "      <th>FENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>Gable</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>TA</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>Gd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>RFn</td>\n",
       "      <td>TA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>Gable</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>TA</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>RFn</td>\n",
       "      <td>TA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>Gable</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>TA</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>RFn</td>\n",
       "      <td>TA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>Gable</td>\n",
       "      <td>TA</td>\n",
       "      <td>BrkTil</td>\n",
       "      <td>Gd</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Detchd</td>\n",
       "      <td>Unf</td>\n",
       "      <td>TA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>Gable</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>TA</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>RFn</td>\n",
       "      <td>TA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BLDGTYPE HOUSESTYLE ROOFSTYLE EXTERCOND FOUNDATION BSMTCOND HEATING  \\\n",
       "0     1Fam     2Story     Gable        TA      PConc       TA    GasA   \n",
       "1     1Fam     1Story     Gable        TA     CBlock       TA    GasA   \n",
       "2     1Fam     2Story     Gable        TA      PConc       TA    GasA   \n",
       "3     1Fam     2Story     Gable        TA     BrkTil       Gd    GasA   \n",
       "4     1Fam     2Story     Gable        TA      PConc       TA    GasA   \n",
       "\n",
       "  HEATINGQC CENTRALAIR ELECTRICAL KITCHENQUAL FIREPLACEQU GARAGETYPE  \\\n",
       "0        Ex          Y      SBrkr          Gd         NaN     Attchd   \n",
       "1        Ex          Y      SBrkr          TA          TA     Attchd   \n",
       "2        Ex          Y      SBrkr          Gd          TA     Attchd   \n",
       "3        Gd          Y      SBrkr          Gd          Gd     Detchd   \n",
       "4        Ex          Y      SBrkr          Gd          TA     Attchd   \n",
       "\n",
       "  GARAGEFINISH GARAGECOND POOLQC FENCE  \n",
       "0          RFn         TA    NaN   NaN  \n",
       "1          RFn         TA    NaN   NaN  \n",
       "2          RFn         TA    NaN   NaN  \n",
       "3          Unf         TA    NaN   NaN  \n",
       "4          RFn         TA    NaN   NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[cat_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing NaN values and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_categorical = SimpleImputer(strategy=\"most_frequent\")\n",
    "onehot_categorical = OneHotEncoder(handle_unknown='ignore', sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat_cols] = pd.DataFrame(impute_categorical.fit_transform(df[cat_cols]), columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = pd.DataFrame(onehot_categorical.fit_transform(df[cat_cols]), columns=onehot_categorical.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = pd.concat([df[num_cols], df_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LOTAREA</th>\n",
       "      <th>OVERALLCOND</th>\n",
       "      <th>YEARBUILT</th>\n",
       "      <th>FULLBATH</th>\n",
       "      <th>HALFBATH</th>\n",
       "      <th>BEDROOMABVGR</th>\n",
       "      <th>KITCHENABVGR</th>\n",
       "      <th>TOTRMSABVGRD</th>\n",
       "      <th>FIREPLACES</th>\n",
       "      <th>...</th>\n",
       "      <th>x14_Gd</th>\n",
       "      <th>x14_Po</th>\n",
       "      <th>x14_TA</th>\n",
       "      <th>x15_Ex</th>\n",
       "      <th>x15_Fa</th>\n",
       "      <th>x15_Gd</th>\n",
       "      <th>x16_GdPrv</th>\n",
       "      <th>x16_GdWo</th>\n",
       "      <th>x16_MnPrv</th>\n",
       "      <th>x16_MnWw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8450</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9600</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11250</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9550</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>14260</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LOTAREA  OVERALLCOND  YEARBUILT  FULLBATH  HALFBATH  BEDROOMABVGR  \\\n",
       "0   1     8450            5       2003         2         1             3   \n",
       "1   2     9600            8       1976         2         0             3   \n",
       "2   3    11250            5       2001         2         1             3   \n",
       "3   4     9550            5       1915         1         0             3   \n",
       "4   5    14260            5       2000         2         1             4   \n",
       "\n",
       "   KITCHENABVGR  TOTRMSABVGRD  FIREPLACES  ...  x14_Gd  x14_Po  x14_TA  \\\n",
       "0             1             8           0  ...     0.0     0.0     1.0   \n",
       "1             1             6           1  ...     0.0     0.0     1.0   \n",
       "2             1             6           1  ...     0.0     0.0     1.0   \n",
       "3             1             7           1  ...     0.0     0.0     1.0   \n",
       "4             1             9           1  ...     0.0     0.0     1.0   \n",
       "\n",
       "   x15_Ex  x15_Fa  x15_Gd  x16_GdPrv  x16_GdWo  x16_MnPrv  x16_MnWw  \n",
       "0     0.0     0.0     1.0        0.0       0.0        1.0       0.0  \n",
       "1     0.0     0.0     1.0        0.0       0.0        1.0       0.0  \n",
       "2     0.0     0.0     1.0        0.0       0.0        1.0       0.0  \n",
       "3     0.0     0.0     1.0        0.0       0.0        1.0       0.0  \n",
       "4     0.0     0.0     1.0        0.0       0.0        1.0       0.0  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd.shape\n",
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_names = df_pd.columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = df_pd.drop(['ID','SALEPRICE'], axis=1)\n",
    "y = df_pd['SALEPRICE']\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 97)\n",
      "\n",
      "\n",
      "X_train:  (1022, 95)\n",
      "y_train:  (1022,)\n",
      "X_test:  (438, 95)\n",
      "y_test:  (438,)\n"
     ]
    }
   ],
   "source": [
    "print(df_pd.shape)\n",
    "print(\"\\n\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "#print(\"\\n\")\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import gc\n",
    "from hyperopt import hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "#optional but advised\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#GLOBAL HYPEROPT PARAMETERS\n",
    "NUM_EVALS = 1000 #number of hyperopt evaluation rounds\n",
    "N_FOLDS = 5 #number of cross-validation folds on data in each evaluation round\n",
    "\n",
    "#LIGHTGBM PARAMETERS\n",
    "LGBM_MAX_LEAVES = 2**11 #maximum number of leaves per tree for LightGBM\n",
    "LGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\n",
    "EVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \n",
    "EVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n",
    "\n",
    "#XGBOOST PARAMETERS\n",
    "XGB_MAX_LEAVES = 2**12 #maximum number of leaves when using histogram splitting\n",
    "XGB_MAX_DEPTH = 25 #maximum tree depth for XGBoost\n",
    "EVAL_METRIC_XGB_REG = 'mae' #XGBoost regression metric\n",
    "EVAL_METRIC_XGB_CLASS = 'auc' #XGBoost classification metric\n",
    "\n",
    "#CATBOOST PARAMETERS\n",
    "CB_MAX_DEPTH = 8 #maximum tree depth in CatBoost\n",
    "OBJECTIVE_CB_REG = 'MAE' #CatBoost regression metric\n",
    "OBJECTIVE_CB_CLASS = 'Logloss' #CatBoost classification metric\n",
    "\n",
    "#OPTIONAL OUTPUT\n",
    "BEST_SCORE = 0\n",
    "\n",
    "def quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False):\n",
    "    \n",
    "    #==========\n",
    "    #LightGBM\n",
    "    #==========\n",
    "    \n",
    "    if package=='lgbm':\n",
    "        \n",
    "        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth',\n",
    "                         'num_leaves',\n",
    "                          'max_bin',\n",
    "                         'min_data_in_leaf',\n",
    "                         'min_data_in_bin']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            #cast integer params from float to int\n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "            \n",
    "            #extract nested conditional parameters\n",
    "            if space_params['boosting']['boosting'] == 'goss':\n",
    "                top_rate = space_params['boosting'].get('top_rate')\n",
    "                other_rate = space_params['boosting'].get('other_rate')\n",
    "                #0 <= top_rate + other_rate <= 1\n",
    "                top_rate = max(top_rate, 0)\n",
    "                top_rate = min(top_rate, 0.5)\n",
    "                other_rate = max(other_rate, 0)\n",
    "                other_rate = min(other_rate, 0.5)\n",
    "                space_params['top_rate'] = top_rate\n",
    "                space_params['other_rate'] = other_rate\n",
    "            \n",
    "            subsample = space_params['boosting'].get('subsample', 1.0)\n",
    "            space_params['boosting'] = space_params['boosting']['boosting']\n",
    "            space_params['subsample'] = subsample\n",
    "            \n",
    "            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n",
    "            cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n",
    "                                early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n",
    "            \n",
    "            best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            #best_loss = 1 - cv_results['auc-mean'][-1]\n",
    "            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = lgb.Dataset(data, labels, free_raw_data = False)\n",
    "                \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = [{'boosting': 'gbdt',\n",
    "                          'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                         {'boosting': 'goss',\n",
    "                          'subsample': 1.0,\n",
    "                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n",
    "                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n",
    "        metric_list = ['MAE', 'RMSE'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        #metric_list = ['auc'] #modify as required for other classification metrics\n",
    "        objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n",
    "        objective_list_class = ['binary', 'cross_entropy']\n",
    "        #for classification set objective_list = objective_list_class\n",
    "        objective_list = objective_list_reg\n",
    "\n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n",
    "                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n",
    "                'max_bin': hp.quniform('max_bin', 32, 255, 1),\n",
    "                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n",
    "                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n",
    "                'min_gain_to_split' : hp.quniform('min_gain_to_split', 0.1, 5, 0.01),\n",
    "                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n",
    "                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'metric' : hp.choice('metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n",
    "                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01),\n",
    "                'verbose': -1,\n",
    "                'n_jobs': 1\n",
    "            }\n",
    "        \n",
    "        #optional: activate GPU for LightGBM\n",
    "        #follow compilation steps here:\n",
    "        #https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm/\n",
    "        #then uncomment lines below:\n",
    "        #space['device'] = 'gpu'\n",
    "        #space['gpu_platform_id'] = 0,\n",
    "        #space['gpu_device_id'] =  0\n",
    "\n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "                \n",
    "        #fmin() will return the index of values chosen from the lists/arrays in 'space'\n",
    "        #to obtain actual values, index values are used to subset the original lists/arrays\n",
    "        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n",
    "        best['metric'] = metric_list[best['metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "                \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    #==========\n",
    "    #XGBoost\n",
    "    #==========\n",
    "    \n",
    "    if package=='xgb':\n",
    "        \n",
    "        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "                \n",
    "            #extract multiple nested tree_method conditional parameters\n",
    "            #libera te tutemet ex inferis\n",
    "            if space_params['tree_method']['tree_method'] == 'hist':\n",
    "                max_bin = space_params['tree_method'].get('max_bin')\n",
    "                space_params['max_bin'] = int(max_bin)\n",
    "                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n",
    "                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n",
    "                    space_params['grow_policy'] = grow_policy\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "                else:\n",
    "                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n",
    "                    space_params['grow_policy'] = 'lossguide'\n",
    "                    space_params['max_leaves'] = int(max_leaves)\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "            else:\n",
    "                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n",
    "                \n",
    "            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n",
    "            cv_results = xgb.cv(space_params, train, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_REG],\n",
    "                             early_stopping_rounds=100, stratified=True, seed=42, verbose_eval=0)\n",
    "            \n",
    "            best_loss = cv_results['test-mae-mean'].iloc[-1] #or 'test-rmse-mean' if using RMSE\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            #best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n",
    "            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = xgb.DMatrix(data, labels)\n",
    "        \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n",
    "        metric_list = ['MAE', 'RMSE'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        #metric_list = ['auc']\n",
    "        #modify as required for other classification metrics classification\n",
    "        \n",
    "        tree_method = [{'tree_method' : 'exact'},\n",
    "               {'tree_method' : 'approx'},\n",
    "               {'tree_method' : 'hist',\n",
    "                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n",
    "                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n",
    "                                'grow_policy' : {'grow_policy':'lossguide',\n",
    "                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n",
    "        \n",
    "        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n",
    "        #'gpu_hist' in the nested dictionary above\n",
    "        \n",
    "        objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n",
    "        objective_list_class = ['reg:logistic', 'binary:logistic']\n",
    "        #for classification change line below to 'objective_list = objective_list_class'\n",
    "        objective_list = objective_list_reg\n",
    "        \n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'tree_method' : hp.choice('tree_method', tree_method),\n",
    "                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n",
    "                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n",
    "                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n",
    "                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n",
    "                'gamma' : hp.uniform('gamma', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'eval_metric' : hp.choice('eval_metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n",
    "                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n",
    "                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n",
    "                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "                'nthread' : -1,\n",
    "                'verbosity': 0,\n",
    "            }\n",
    "        \n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "        \n",
    "        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n",
    "        best['boosting'] = boosting_list[best['boosting']]\n",
    "        best['eval_metric'] = metric_list[best['eval_metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "        \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        if 'max_leaves' in best:\n",
    "            best['max_leaves'] = int(best['max_leaves'])\n",
    "        if 'max_bin' in best:\n",
    "            best['max_bin'] = int(best['max_bin'])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        \n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    #==========\n",
    "    #CatBoost\n",
    "    #==========\n",
    "    \n",
    "    if package=='cb':\n",
    "        \n",
    "        print('Running {} rounds of CatBoost parameter optimisation:'.format(num_evals))\n",
    "        \n",
    "        #clear memory \n",
    "        gc.collect()\n",
    "            \n",
    "        integer_params = ['depth',\n",
    "                          #'one_hot_max_size', #for categorical data\n",
    "                          'min_data_in_leaf',\n",
    "                          'max_bin']\n",
    "        \n",
    "        def objective(space_params):\n",
    "                        \n",
    "            #cast integer params from float to int\n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "                \n",
    "            #extract nested conditional parameters\n",
    "            if space_params['bootstrap_type']['bootstrap_type'] == 'Bayesian':\n",
    "                bagging_temp = space_params['bootstrap_type'].get('bagging_temperature')\n",
    "                space_params['bagging_temperature'] = bagging_temp\n",
    "                \n",
    "            if space_params['grow_policy']['grow_policy'] == 'LossGuide':\n",
    "                max_leaves = space_params['grow_policy'].get('max_leaves')\n",
    "                space_params['max_leaves'] = int(max_leaves)\n",
    "                \n",
    "            space_params['bootstrap_type'] = space_params['bootstrap_type']['bootstrap_type']\n",
    "            space_params['grow_policy'] = space_params['grow_policy']['grow_policy']\n",
    "                           \n",
    "            #random_strength cannot be < 0\n",
    "            space_params['random_strength'] = max(space_params['random_strength'], 0)\n",
    "            #fold_len_multiplier cannot be < 1\n",
    "            space_params['fold_len_multiplier'] = max(space_params['fold_len_multiplier'], 1)\n",
    "                       \n",
    "            #for classification set stratified=True\n",
    "            cv_results = cb.cv(train, space_params, fold_count=N_FOLDS, \n",
    "                             early_stopping_rounds=25, stratified=True, partition_random_seed=42)\n",
    "           \n",
    "            #best_loss = cv_results['test-MAE-mean'].iloc[-1] #'test-RMSE-mean' for RMSE\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            best_loss = cv_results['test-Logloss-mean'].iloc[-1]\n",
    "            #if necessary, replace 'test-Logloss-mean' with 'test-[your-preferred-metric]-mean'\n",
    "            \n",
    "            return{'loss':best_loss, 'status': STATUS_OK}\n",
    "        \n",
    "        train = cb.Pool(data, labels.astype('float32'))\n",
    "        \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        bootstrap_type = [#{'bootstrap_type':'Poisson'}, \n",
    "                           {'bootstrap_type':'Bayesian',\n",
    "                            'bagging_temperature' : hp.loguniform('bagging_temperature', np.log(1), np.log(50))},\n",
    "                          {'bootstrap_type':'Bernoulli'}] \n",
    "        LEB = ['No', 'AnyImprovement'] #remove 'Armijo' if not using GPU\n",
    "        #score_function = ['Correlation', 'L2', 'NewtonCorrelation', 'NewtonL2']\n",
    "        grow_policy = [{'grow_policy':'SymmetricTree'},\n",
    "                       {'grow_policy':'Depthwise'},\n",
    "                       {'grow_policy':'Lossguide',\n",
    "                        'max_leaves': hp.quniform('max_leaves', 2, 32, 1)}]\n",
    "        eval_metric_list_reg = ['MAE', 'RMSE']\n",
    "        eval_metric_list_class = ['Logloss', 'AUC']\n",
    "        #for classification change line below to 'eval_metric_list = eval_metric_list_class'\n",
    "        eval_metric_list = eval_metric_list_class\n",
    "                \n",
    "        space ={'depth': hp.quniform('depth', 2, CB_MAX_DEPTH, 1),\n",
    "                'max_bin' : hp.quniform('max_bin', 1, 32, 1), #if using CPU just set this to 254\n",
    "                'l2_leaf_reg' : hp.uniform('l2_leaf_reg', 0, 5),\n",
    "                'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 1, 50, 1),\n",
    "                'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n",
    "                #'one_hot_max_size' : hp.quniform('one_hot_max_size', 2, 16, 1), #uncomment if using categorical features\n",
    "                'bootstrap_type' : hp.choice('bootstrap_type', bootstrap_type),\n",
    "                'learning_rate' : hp.uniform('learning_rate', 0.05, 0.25),\n",
    "                'eval_metric' : hp.choice('eval_metric', eval_metric_list),\n",
    "                'objective' : OBJECTIVE_CB_CLASS,\n",
    "                #'score_function' : hp.choice('score_function', score_function), #crashes kernel - reason unknown\n",
    "                'leaf_estimation_backtracking' : hp.choice('leaf_estimation_backtracking', LEB),\n",
    "                'grow_policy': hp.choice('grow_policy', grow_policy),\n",
    "                #'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),# CPU only\n",
    "                'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n",
    "                'od_type' : 'Iter',\n",
    "                'od_wait' : 25,\n",
    "                'task_type' : 'GPU',\n",
    "                'verbose' : 0,\n",
    "            }\n",
    "        \n",
    "        #optional: run CatBoost without GPU\n",
    "        #uncomment line below\n",
    "        space['task_type'] = 'CPU'\n",
    "            \n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "        \n",
    "        #unpack nested dicts first\n",
    "        best['bootstrap_type'] = bootstrap_type[best['bootstrap_type']]['bootstrap_type']\n",
    "        best['grow_policy'] = grow_policy[best['grow_policy']]['grow_policy']\n",
    "        best['eval_metric'] = eval_metric_list[best['eval_metric']]\n",
    "        \n",
    "        #best['score_function'] = score_function[best['score_function']] \n",
    "        #best['leaf_estimation_method'] = LEM[best['leaf_estimation_method']] #CPU only\n",
    "        best['leaf_estimation_backtracking'] = LEB[best['leaf_estimation_backtracking']]        \n",
    "        \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        if 'max_leaves' in best:\n",
    "            best['max_leaves'] = int(best['max_leaves'])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        \n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    else:\n",
    "        print('Package not recognised. Please use \"lgbm\" for LightGBM, \"xgb\" for XGBoost or \"cb\" for CatBoost.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1000 rounds of XGBoost parameter optimisation:\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [05:17<00:00,  3.15trial/s, best loss: 28140.5308594]\n",
      "{boosting: gbtree\n",
      "colsample_bylevel: 0.52\n",
      "colsample_bynode: 0.64\n",
      "colsample_bytree: 0.64\n",
      "eval_metric: RMSE\n",
      "gamma: 0.4025242673403483\n",
      "learning_rate: 0.19960440670066074\n",
      "max_depth: 8\n",
      "min_child_weight: 4.832441995554798\n",
      "objective: reg:linear\n",
      "reg_alpha: 2.1471799927248414\n",
      "reg_lambda: 0.004916348867284151\n",
      "subsample: 0.9500000000000001\n",
      "tree_method: exact}\n"
     ]
    }
   ],
   "source": [
    "xgb_params = quick_hyperopt(X_train, y_train, 'xgb', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting': 'gbtree',\n",
       " 'colsample_bylevel': 0.52,\n",
       " 'colsample_bynode': 0.64,\n",
       " 'colsample_bytree': 0.64,\n",
       " 'eval_metric': 'RMSE',\n",
       " 'gamma': 0.4025242673403483,\n",
       " 'learning_rate': 0.19960440670066074,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 4.832441995554798,\n",
       " 'objective': 'reg:linear',\n",
       " 'reg_alpha': 2.1471799927248414,\n",
       " 'reg_lambda': 0.004916348867284151,\n",
       " 'subsample': 0.9500000000000001,\n",
       " 'tree_method': 'exact'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'boosting': 'gbtree',\n",
    "    'colsample_bylevel': 0.52,\n",
    "    'colsample_bynode': 0.64,\n",
    "    'colsample_bytree': 0.64,\n",
    "    'eval_metric': 'rmse',\n",
    "    'gamma': 0.4025242673403483,\n",
    "    'learning_rate': 0.19960440670066074,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 4.832441995554798,\n",
    "    'objective': 'reg:linear',\n",
    "    'reg_alpha': 2.1471799927248414,\n",
    "    'reg_lambda': 0.004916348867284151,\n",
    "    'subsample': 0.9500000000000001,\n",
    "    'tree_method': 'exact'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:159292.73438\tvalidation_1-rmse:161079.79688\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-rmse:129286.82812\tvalidation_1-rmse:132362.03125\n",
      "[2]\tvalidation_0-rmse:105374.43750\tvalidation_1-rmse:108899.24219\n",
      "[3]\tvalidation_0-rmse:86544.04688\tvalidation_1-rmse:90758.95312\n",
      "[4]\tvalidation_0-rmse:71596.00781\tvalidation_1-rmse:76619.03906\n",
      "[5]\tvalidation_0-rmse:59698.97656\tvalidation_1-rmse:65127.64453\n",
      "[6]\tvalidation_0-rmse:50439.01953\tvalidation_1-rmse:57296.27734\n",
      "[7]\tvalidation_0-rmse:43297.63672\tvalidation_1-rmse:50702.06250\n",
      "[8]\tvalidation_0-rmse:37543.15234\tvalidation_1-rmse:45993.57422\n",
      "[9]\tvalidation_0-rmse:33135.32422\tvalidation_1-rmse:43100.21875\n",
      "[10]\tvalidation_0-rmse:30148.29102\tvalidation_1-rmse:41062.36719\n",
      "[11]\tvalidation_0-rmse:27580.50977\tvalidation_1-rmse:39254.04688\n",
      "[12]\tvalidation_0-rmse:25860.95508\tvalidation_1-rmse:38185.21094\n",
      "[13]\tvalidation_0-rmse:24182.75391\tvalidation_1-rmse:37345.88281\n",
      "[14]\tvalidation_0-rmse:23042.43555\tvalidation_1-rmse:37094.35547\n",
      "[15]\tvalidation_0-rmse:21967.84766\tvalidation_1-rmse:36465.52344\n",
      "[16]\tvalidation_0-rmse:21424.39258\tvalidation_1-rmse:36169.28125\n",
      "[17]\tvalidation_0-rmse:20583.93555\tvalidation_1-rmse:35780.03125\n",
      "[18]\tvalidation_0-rmse:19696.82812\tvalidation_1-rmse:35721.74219\n",
      "[19]\tvalidation_0-rmse:19282.41602\tvalidation_1-rmse:35681.23047\n",
      "[20]\tvalidation_0-rmse:18739.60547\tvalidation_1-rmse:35522.55859\n",
      "[21]\tvalidation_0-rmse:17957.81836\tvalidation_1-rmse:35173.17578\n",
      "[22]\tvalidation_0-rmse:17491.27344\tvalidation_1-rmse:35192.34766\n",
      "[23]\tvalidation_0-rmse:17150.25000\tvalidation_1-rmse:34997.98828\n",
      "[24]\tvalidation_0-rmse:16861.34961\tvalidation_1-rmse:34960.06641\n",
      "[25]\tvalidation_0-rmse:16523.29102\tvalidation_1-rmse:35205.00391\n",
      "[26]\tvalidation_0-rmse:16282.74707\tvalidation_1-rmse:35192.76172\n",
      "[27]\tvalidation_0-rmse:16048.77734\tvalidation_1-rmse:35153.93750\n",
      "[28]\tvalidation_0-rmse:15931.01562\tvalidation_1-rmse:35118.64844\n",
      "[29]\tvalidation_0-rmse:15775.99707\tvalidation_1-rmse:35093.17969\n",
      "Stopping. Best iteration:\n",
      "[24]\tvalidation_0-rmse:16861.34961\tvalidation_1-rmse:34960.06641\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', boosting='gbtree',\n",
       "             colsample_bylevel=0.52, colsample_bynode=0.64,\n",
       "             colsample_bytree=0.64, eval_metric='rmse',\n",
       "             gamma=0.4025242673403483, gpu_id=-1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.19960440670066074,\n",
       "             max_delta_step=0, max_depth=8, min_child_weight=4.832441995554798,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=2.1471799927248414, reg_lambda=0.004916348867284151,\n",
       "             scale_pos_weight=1, subsample=0.9500000000000001,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_xgb = xgb.XGBRegressor()\n",
    "reg_xgb.set_params(**xgb_params)\n",
    "reg_xgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    208500\n",
       "1    181500\n",
       "Name: SALEPRICE, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd.head(2)['SALEPRICE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([202224.55, 175889.7 ], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_xgb.predict(X[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34960.065385903836"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "y_pred_te = reg_xgb.predict(X_test)\n",
    "sqrt(mean_squared_error(y_test, y_pred_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1000 rounds of LightGBM parameter optimisation:\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [07:47<00:00,  2.14trial/s, best loss: 22635.80794816267]\n",
      "{bagging_fraction: 0.84\n",
      "boosting: gbdt\n",
      "feature_fraction: 0.67\n",
      "lambda_l1: 3.0904688104661124\n",
      "lambda_l2: 0.10041209033823585\n",
      "learning_rate: 0.039574312573301115\n",
      "max_bin: 120\n",
      "max_depth: 7\n",
      "metric: MAE\n",
      "min_data_in_bin: 42\n",
      "min_data_in_leaf: 1\n",
      "min_gain_to_split: 4.5600000000000005\n",
      "num_leaves: 1487\n",
      "objective: tweedie\n",
      "subsample: 0.6481492939476654}\n"
     ]
    }
   ],
   "source": [
    "lgb_params = quick_hyperopt(X_train, y_train, 'lgbm', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.84,\n",
       " 'boosting': 'gbdt',\n",
       " 'feature_fraction': 0.67,\n",
       " 'lambda_l1': 3.0904688104661124,\n",
       " 'lambda_l2': 0.10041209033823585,\n",
       " 'learning_rate': 0.039574312573301115,\n",
       " 'max_bin': 120,\n",
       " 'max_depth': 7,\n",
       " 'metric': 'MAE',\n",
       " 'min_data_in_bin': 42,\n",
       " 'min_data_in_leaf': 1,\n",
       " 'min_gain_to_split': 4.5600000000000005,\n",
       " 'num_leaves': 1487,\n",
       " 'objective': 'tweedie',\n",
       " 'subsample': 0.6481492939476654}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'bagging_fraction': 0.84,\n",
    "    'boosting': 'gbdt',\n",
    "    'feature_fraction': 0.67,\n",
    "    'lambda_l1': 3.0904688104661124,\n",
    "    'lambda_l2': 0.10041209033823585,\n",
    "    'learning_rate': 0.039574312573301115,\n",
    "    'max_bin': 120,\n",
    "    'max_depth': 7,\n",
    "    'metric': 'MAE',\n",
    "    'min_data_in_bin': 42,\n",
    "    'min_data_in_leaf': 1,\n",
    "    'min_gain_to_split': 4.5600000000000005,\n",
    "    'num_leaves': 1487,\n",
    "    'objective': 'tweedie',\n",
    "    'subsample': 0.6481492939476654\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.67, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.67\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=4.5600000000000005, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=4.5600000000000005\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.0904688104661124, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.0904688104661124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.84, subsample=0.6481492939476654 will be ignored. Current value: bagging_fraction=0.84\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10041209033823585, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10041209033823585\n",
      "[1]\tvalid_0's l1: 57670.1\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's l1: 55924.3\n",
      "[3]\tvalid_0's l1: 54224.5\n",
      "[4]\tvalid_0's l1: 52570.7\n",
      "[5]\tvalid_0's l1: 51108.8\n",
      "[6]\tvalid_0's l1: 49656.5\n",
      "[7]\tvalid_0's l1: 48372.5\n",
      "[8]\tvalid_0's l1: 47071.4\n",
      "[9]\tvalid_0's l1: 45749.5\n",
      "[10]\tvalid_0's l1: 44531.9\n",
      "[11]\tvalid_0's l1: 43353.1\n",
      "[12]\tvalid_0's l1: 42275.3\n",
      "[13]\tvalid_0's l1: 41184.5\n",
      "[14]\tvalid_0's l1: 40252.9\n",
      "[15]\tvalid_0's l1: 39283.1\n",
      "[16]\tvalid_0's l1: 38320.9\n",
      "[17]\tvalid_0's l1: 37504.6\n",
      "[18]\tvalid_0's l1: 36652\n",
      "[19]\tvalid_0's l1: 35869.8\n",
      "[20]\tvalid_0's l1: 35144.2\n",
      "[21]\tvalid_0's l1: 34531.1\n",
      "[22]\tvalid_0's l1: 33800.7\n",
      "[23]\tvalid_0's l1: 33154.7\n",
      "[24]\tvalid_0's l1: 32571.2\n",
      "[25]\tvalid_0's l1: 32042\n",
      "[26]\tvalid_0's l1: 31506.4\n",
      "[27]\tvalid_0's l1: 31014.7\n",
      "[28]\tvalid_0's l1: 30521.1\n",
      "[29]\tvalid_0's l1: 30074.6\n",
      "[30]\tvalid_0's l1: 29639.6\n",
      "[31]\tvalid_0's l1: 29199.5\n",
      "[32]\tvalid_0's l1: 28803.3\n",
      "[33]\tvalid_0's l1: 28443.6\n",
      "[34]\tvalid_0's l1: 28118.1\n",
      "[35]\tvalid_0's l1: 27769.7\n",
      "[36]\tvalid_0's l1: 27392.2\n",
      "[37]\tvalid_0's l1: 27061\n",
      "[38]\tvalid_0's l1: 26815.3\n",
      "[39]\tvalid_0's l1: 26554\n",
      "[40]\tvalid_0's l1: 26323.4\n",
      "[41]\tvalid_0's l1: 26137.5\n",
      "[42]\tvalid_0's l1: 25918.1\n",
      "[43]\tvalid_0's l1: 25729.8\n",
      "[44]\tvalid_0's l1: 25530\n",
      "[45]\tvalid_0's l1: 25343.8\n",
      "[46]\tvalid_0's l1: 25150.5\n",
      "[47]\tvalid_0's l1: 25016.4\n",
      "[48]\tvalid_0's l1: 24857.1\n",
      "[49]\tvalid_0's l1: 24723\n",
      "[50]\tvalid_0's l1: 24587\n",
      "[51]\tvalid_0's l1: 24471.7\n",
      "[52]\tvalid_0's l1: 24338.9\n",
      "[53]\tvalid_0's l1: 24218.1\n",
      "[54]\tvalid_0's l1: 24103.5\n",
      "[55]\tvalid_0's l1: 24017.6\n",
      "[56]\tvalid_0's l1: 23925\n",
      "[57]\tvalid_0's l1: 23848.9\n",
      "[58]\tvalid_0's l1: 23772.2\n",
      "[59]\tvalid_0's l1: 23708.6\n",
      "[60]\tvalid_0's l1: 23673.1\n",
      "[61]\tvalid_0's l1: 23591.9\n",
      "[62]\tvalid_0's l1: 23542.6\n",
      "[63]\tvalid_0's l1: 23501.4\n",
      "[64]\tvalid_0's l1: 23455.5\n",
      "[65]\tvalid_0's l1: 23413.8\n",
      "[66]\tvalid_0's l1: 23371.3\n",
      "[67]\tvalid_0's l1: 23339.5\n",
      "[68]\tvalid_0's l1: 23304.2\n",
      "[69]\tvalid_0's l1: 23274.4\n",
      "[70]\tvalid_0's l1: 23238.7\n",
      "[71]\tvalid_0's l1: 23200.3\n",
      "[72]\tvalid_0's l1: 23159.1\n",
      "[73]\tvalid_0's l1: 23118.1\n",
      "[74]\tvalid_0's l1: 23093.9\n",
      "[75]\tvalid_0's l1: 23066.4\n",
      "[76]\tvalid_0's l1: 23048.3\n",
      "[77]\tvalid_0's l1: 23021.1\n",
      "[78]\tvalid_0's l1: 23004.4\n",
      "[79]\tvalid_0's l1: 22989.5\n",
      "[80]\tvalid_0's l1: 22966.5\n",
      "[81]\tvalid_0's l1: 22927.1\n",
      "[82]\tvalid_0's l1: 22898.3\n",
      "[83]\tvalid_0's l1: 22888.5\n",
      "[84]\tvalid_0's l1: 22885.8\n",
      "[85]\tvalid_0's l1: 22863.4\n",
      "[86]\tvalid_0's l1: 22849.1\n",
      "[87]\tvalid_0's l1: 22838.7\n",
      "[88]\tvalid_0's l1: 22823.7\n",
      "[89]\tvalid_0's l1: 22806.4\n",
      "[90]\tvalid_0's l1: 22796.5\n",
      "[91]\tvalid_0's l1: 22790.8\n",
      "[92]\tvalid_0's l1: 22769.3\n",
      "[93]\tvalid_0's l1: 22757.3\n",
      "[94]\tvalid_0's l1: 22748.4\n",
      "[95]\tvalid_0's l1: 22739.6\n",
      "[96]\tvalid_0's l1: 22744\n",
      "[97]\tvalid_0's l1: 22730.5\n",
      "[98]\tvalid_0's l1: 22722.5\n",
      "[99]\tvalid_0's l1: 22726\n",
      "[100]\tvalid_0's l1: 22731.6\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's l1: 22722.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.84, boosting='gbdt', feature_fraction=0.67,\n",
       "              lambda_l1=3.0904688104661124, lambda_l2=0.10041209033823585,\n",
       "              learning_rate=0.039574312573301115, max_bin=120, max_depth=7,\n",
       "              metric='MAE', min_data_in_bin=42, min_data_in_leaf=1,\n",
       "              min_gain_to_split=4.5600000000000005, num_leaves=1487,\n",
       "              objective='tweedie', subsample=0.6481492939476654)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_lgb = lgb.LGBMRegressor()\n",
    "reg_lgb.set_params(**lgb_params) \n",
    "reg_lgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34899.1627728025"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "y_pred_te = reg_lgb.predict(X_test)\n",
    "sqrt(mean_squared_error(y_test, y_pred_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "epoch 0  | loss: 38892769280.0| train_rmse: 197202.74068| valid_rmse: 198436.50396|  0:00:00s\n",
      "epoch 1  | loss: 38890680320.0| train_rmse: 197185.38462| valid_rmse: 198418.8346|  0:00:00s\n",
      "epoch 2  | loss: 38887477248.0| train_rmse: 197157.531| valid_rmse: 198391.81905|  0:00:01s\n",
      "epoch 3  | loss: 38883151872.0| train_rmse: 197122.87692| valid_rmse: 198356.18599|  0:00:01s\n",
      "epoch 4  | loss: 38877188096.0| train_rmse: 197065.77806| valid_rmse: 198300.82247|  0:00:01s\n",
      "epoch 5  | loss: 38868578304.0| train_rmse: 196919.98851| valid_rmse: 198156.75754|  0:00:02s\n",
      "epoch 6  | loss: 38858465280.0| train_rmse: 196733.13057| valid_rmse: 197949.36374|  0:00:02s\n",
      "epoch 7  | loss: 38845718528.0| train_rmse: 196623.0619| valid_rmse: 197833.66564|  0:00:03s\n",
      "epoch 8  | loss: 38830223360.0| train_rmse: 196391.85345| valid_rmse: 197577.09732|  0:00:03s\n",
      "epoch 9  | loss: 38812409856.0| train_rmse: 196202.01396| valid_rmse: 197389.37047|  0:00:03s\n",
      "epoch 10 | loss: 38791589888.0| train_rmse: 195992.03326| valid_rmse: 197193.43183|  0:00:04s\n",
      "epoch 11 | loss: 38768386048.0| train_rmse: 195856.74636| valid_rmse: 197069.54385|  0:00:04s\n",
      "epoch 12 | loss: 38743097344.0| train_rmse: 195921.25226| valid_rmse: 197129.6265|  0:00:04s\n",
      "epoch 13 | loss: 38715502592.0| train_rmse: 195638.44036| valid_rmse: 196862.97204|  0:00:05s\n",
      "epoch 14 | loss: 38683496448.0| train_rmse: 195671.21372| valid_rmse: 196873.35729|  0:00:05s\n",
      "epoch 15 | loss: 38649331712.0| train_rmse: 195533.12492| valid_rmse: 196739.83076|  0:00:06s\n",
      "epoch 16 | loss: 38610874368.0| train_rmse: 195363.05611| valid_rmse: 196588.34943|  0:00:06s\n",
      "epoch 17 | loss: 38569971712.0| train_rmse: 195285.60995| valid_rmse: 196508.3886|  0:00:07s\n",
      "epoch 18 | loss: 38526406656.0| train_rmse: 195209.01234| valid_rmse: 196411.58314|  0:00:07s\n",
      "epoch 19 | loss: 38480359424.0| train_rmse: 194791.36756| valid_rmse: 196010.89289|  0:00:08s\n",
      "epoch 20 | loss: 38429802496.0| train_rmse: 194371.40125| valid_rmse: 195529.76881|  0:00:08s\n",
      "epoch 21 | loss: 38376890368.0| train_rmse: 193903.89899| valid_rmse: 195086.16561|  0:00:08s\n",
      "epoch 22 | loss: 38319050752.0| train_rmse: 193790.58232| valid_rmse: 194987.2375|  0:00:09s\n",
      "epoch 23 | loss: 38262640640.0| train_rmse: 193968.13193| valid_rmse: 195188.13937|  0:00:09s\n",
      "epoch 24 | loss: 38197207040.0| train_rmse: 194140.08963| valid_rmse: 195378.45498|  0:00:09s\n",
      "epoch 25 | loss: 38133395456.0| train_rmse: 194057.39293| valid_rmse: 195282.06754|  0:00:10s\n",
      "epoch 26 | loss: 38065205248.0| train_rmse: 193832.78716| valid_rmse: 195048.50926|  0:00:10s\n",
      "epoch 27 | loss: 37993205760.0| train_rmse: 193624.40748| valid_rmse: 194893.07793|  0:00:11s\n",
      "epoch 28 | loss: 37918330880.0| train_rmse: 193304.21268| valid_rmse: 194552.42653|  0:00:11s\n",
      "epoch 29 | loss: 37852909568.0| train_rmse: 193419.6834| valid_rmse: 194716.94304|  0:00:11s\n",
      "epoch 30 | loss: 37761163264.0| train_rmse: 193245.37722| valid_rmse: 194566.89296|  0:00:12s\n",
      "epoch 31 | loss: 37681004544.0| train_rmse: 193066.74759| valid_rmse: 194411.37241|  0:00:12s\n",
      "epoch 32 | loss: 37594279936.0| train_rmse: 192952.87865| valid_rmse: 194246.85916|  0:00:13s\n",
      "epoch 33 | loss: 37505908736.0| train_rmse: 192861.67522| valid_rmse: 194125.85931|  0:00:13s\n",
      "epoch 34 | loss: 37417213952.0| train_rmse: 192703.49889| valid_rmse: 193995.67738|  0:00:13s\n",
      "epoch 35 | loss: 37324976128.0| train_rmse: 191778.76546| valid_rmse: 193020.0684|  0:00:14s\n",
      "epoch 36 | loss: 37223591936.0| train_rmse: 190839.38778| valid_rmse: 192093.55848|  0:00:14s\n",
      "epoch 37 | loss: 37123395584.0| train_rmse: 190476.61594| valid_rmse: 191760.17848|  0:00:14s\n",
      "epoch 38 | loss: 37025693696.0| train_rmse: 189945.13425| valid_rmse: 191233.89305|  0:00:15s\n",
      "epoch 39 | loss: 36913233920.0| train_rmse: 189482.42078| valid_rmse: 190809.92664|  0:00:15s\n",
      "epoch 40 | loss: 36805578752.0| train_rmse: 189081.31427| valid_rmse: 190414.5029|  0:00:16s\n",
      "epoch 41 | loss: 36693553152.0| train_rmse: 188625.51648| valid_rmse: 189956.75887|  0:00:16s\n",
      "epoch 42 | loss: 36575924224.0| train_rmse: 188062.36157| valid_rmse: 189413.45907|  0:00:17s\n",
      "epoch 43 | loss: 36452204544.0| train_rmse: 187762.60386| valid_rmse: 189208.88516|  0:00:17s\n",
      "epoch 44 | loss: 36327120896.0| train_rmse: 187205.71908| valid_rmse: 188695.27559|  0:00:17s\n",
      "epoch 45 | loss: 36202545152.0| train_rmse: 186701.27014| valid_rmse: 188208.30718|  0:00:18s\n",
      "epoch 46 | loss: 36070526976.0| train_rmse: 186191.59212| valid_rmse: 187657.1397|  0:00:18s\n",
      "epoch 47 | loss: 35934306304.0| train_rmse: 185511.5033| valid_rmse: 187025.88079|  0:00:19s\n",
      "epoch 48 | loss: 35792801792.0| train_rmse: 184793.66659| valid_rmse: 186373.29898|  0:00:20s\n",
      "epoch 49 | loss: 35659767808.0| train_rmse: 184060.16088| valid_rmse: 185696.37199|  0:00:20s\n",
      "epoch 50 | loss: 35533164544.0| train_rmse: 183543.16484| valid_rmse: 185006.35168|  0:00:21s\n",
      "epoch 51 | loss: 35389165568.0| train_rmse: 183405.63373| valid_rmse: 184991.61264|  0:00:21s\n",
      "epoch 52 | loss: 35230924800.0| train_rmse: 183280.77427| valid_rmse: 184802.09976|  0:00:22s\n",
      "epoch 53 | loss: 35082452992.0| train_rmse: 182993.47328| valid_rmse: 184507.10349|  0:00:22s\n",
      "epoch 54 | loss: 34925977600.0| train_rmse: 182507.19233| valid_rmse: 183978.04424|  0:00:23s\n",
      "epoch 55 | loss: 34764312576.0| train_rmse: 183049.76643| valid_rmse: 184407.11235|  0:00:23s\n",
      "epoch 56 | loss: 34616672256.0| train_rmse: 181949.63435| valid_rmse: 183373.35924|  0:00:23s\n",
      "epoch 57 | loss: 34476539904.0| train_rmse: 180859.05| valid_rmse: 182506.74668|  0:00:24s\n",
      "epoch 58 | loss: 34327101440.0| train_rmse: 180163.44063| valid_rmse: 181806.00345|  0:00:24s\n",
      "epoch 59 | loss: 34156865536.0| train_rmse: 179553.15551| valid_rmse: 181300.02229|  0:00:25s\n",
      "epoch 60 | loss: 33980856320.0| train_rmse: 179031.068| valid_rmse: 180671.77034|  0:00:25s\n",
      "epoch 61 | loss: 33802573824.0| train_rmse: 178547.29275| valid_rmse: 180240.46771|  0:00:25s\n",
      "epoch 62 | loss: 33628198912.0| train_rmse: 178020.87297| valid_rmse: 179717.44767|  0:00:26s\n",
      "epoch 63 | loss: 33441843200.0| train_rmse: 178515.55092| valid_rmse: 180124.68225|  0:00:26s\n",
      "epoch 64 | loss: 33263308800.0| train_rmse: 177848.15217| valid_rmse: 179509.14332|  0:00:27s\n",
      "epoch 65 | loss: 33060145152.0| train_rmse: 177545.2083| valid_rmse: 179219.6169|  0:00:27s\n",
      "epoch 66 | loss: 32876480512.0| train_rmse: 177786.76359| valid_rmse: 179566.83574|  0:00:27s\n",
      "epoch 67 | loss: 32669892608.0| train_rmse: 177253.45341| valid_rmse: 179107.05902|  0:00:28s\n",
      "epoch 68 | loss: 32482807808.0| train_rmse: 176834.12621| valid_rmse: 178609.13875|  0:00:28s\n",
      "epoch 69 | loss: 32301225984.0| train_rmse: 176152.3461| valid_rmse: 177998.94218|  0:00:29s\n",
      "epoch 70 | loss: 32115654656.0| train_rmse: 175674.87018| valid_rmse: 177504.65317|  0:00:29s\n",
      "epoch 71 | loss: 31919605760.0| train_rmse: 174957.58853| valid_rmse: 176644.52599|  0:00:30s\n",
      "epoch 72 | loss: 31716407296.0| train_rmse: 174293.00993| valid_rmse: 176110.69549|  0:00:30s\n",
      "epoch 73 | loss: 31518130176.0| train_rmse: 173802.61321| valid_rmse: 175784.83842|  0:00:30s\n",
      "epoch 74 | loss: 31314352128.0| train_rmse: 173243.68509| valid_rmse: 175080.34487|  0:00:31s\n",
      "epoch 75 | loss: 31095953408.0| train_rmse: 172644.32717| valid_rmse: 174512.62309|  0:00:31s\n",
      "epoch 76 | loss: 30890049536.0| train_rmse: 172272.20136| valid_rmse: 173826.76022|  0:00:32s\n",
      "epoch 77 | loss: 30646716416.0| train_rmse: 170926.43408| valid_rmse: 172516.08617|  0:00:32s\n",
      "epoch 78 | loss: 30382645248.0| train_rmse: 169187.71643| valid_rmse: 170785.34409|  0:00:33s\n",
      "epoch 79 | loss: 30159624192.0| train_rmse: 167490.73952| valid_rmse: 169199.39337|  0:00:33s\n",
      "epoch 80 | loss: 29913395200.0| train_rmse: 165821.86309| valid_rmse: 167547.53896|  0:00:34s\n",
      "epoch 81 | loss: 29671950336.0| train_rmse: 164967.42808| valid_rmse: 166402.05858|  0:00:34s\n",
      "epoch 82 | loss: 29473533952.0| train_rmse: 164003.78632| valid_rmse: 165521.00792|  0:00:35s\n",
      "epoch 83 | loss: 29247072256.0| train_rmse: 163309.11052| valid_rmse: 165067.71777|  0:00:35s\n",
      "epoch 84 | loss: 29013391360.0| train_rmse: 162582.62888| valid_rmse: 164209.23309|  0:00:35s\n",
      "epoch 85 | loss: 28801202176.0| train_rmse: 161864.59499| valid_rmse: 163440.83731|  0:00:36s\n",
      "epoch 86 | loss: 28587544576.0| train_rmse: 160153.8773| valid_rmse: 161708.53251|  0:00:36s\n",
      "epoch 87 | loss: 28335054848.0| train_rmse: 158939.53363| valid_rmse: 160514.60629|  0:00:37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88 | loss: 28109084672.0| train_rmse: 158167.47086| valid_rmse: 159772.22512|  0:00:37s\n",
      "epoch 89 | loss: 27878967296.0| train_rmse: 158037.15376| valid_rmse: 159700.99775|  0:00:37s\n",
      "epoch 90 | loss: 27620061184.0| train_rmse: 157909.04688| valid_rmse: 159572.81102|  0:00:38s\n",
      "epoch 91 | loss: 27344791552.0| train_rmse: 158135.75482| valid_rmse: 159932.12879|  0:00:38s\n",
      "epoch 92 | loss: 27114770432.0| train_rmse: 158164.86463| valid_rmse: 160008.81637|  0:00:39s\n",
      "epoch 93 | loss: 26859554816.0| train_rmse: 156848.23699| valid_rmse: 158862.0073|  0:00:39s\n",
      "epoch 94 | loss: 26585108480.0| train_rmse: 155412.84951| valid_rmse: 157434.44539|  0:00:39s\n",
      "epoch 95 | loss: 26348267520.0| train_rmse: 154165.59538| valid_rmse: 156051.32332|  0:00:40s\n",
      "epoch 96 | loss: 26090328064.0| train_rmse: 152759.38928| valid_rmse: 154634.148|  0:00:40s\n",
      "epoch 97 | loss: 25850636288.0| train_rmse: 151754.61727| valid_rmse: 153665.00195|  0:00:41s\n",
      "epoch 98 | loss: 25564751872.0| train_rmse: 151080.98259| valid_rmse: 152965.17321|  0:00:41s\n",
      "epoch 99 | loss: 25278908416.0| train_rmse: 150539.8507| valid_rmse: 152424.48212|  0:00:41s\n",
      "epoch 100| loss: 25010872320.0| train_rmse: 150039.45883| valid_rmse: 152088.39478|  0:00:42s\n",
      "epoch 101| loss: 24723456000.0| train_rmse: 148876.94825| valid_rmse: 151131.68601|  0:00:42s\n",
      "epoch 102| loss: 24471719936.0| train_rmse: 147371.41037| valid_rmse: 149727.32233|  0:00:43s\n",
      "epoch 103| loss: 24162772992.0| train_rmse: 145952.77473| valid_rmse: 148455.71997|  0:00:43s\n",
      "epoch 104| loss: 23923806208.0| train_rmse: 144594.19474| valid_rmse: 147232.43919|  0:00:43s\n",
      "epoch 105| loss: 23632457728.0| train_rmse: 143959.4738| valid_rmse: 146511.20173|  0:00:44s\n",
      "epoch 106| loss: 23354880000.0| train_rmse: 142910.53153| valid_rmse: 145675.2649|  0:00:44s\n",
      "epoch 107| loss: 23120306176.0| train_rmse: 142987.30383| valid_rmse: 145819.59623|  0:00:45s\n",
      "epoch 108| loss: 22887987200.0| train_rmse: 143047.2428| valid_rmse: 145876.13179|  0:00:45s\n",
      "epoch 109| loss: 22607751168.0| train_rmse: 141723.95677| valid_rmse: 144583.00483|  0:00:45s\n",
      "epoch 110| loss: 22346409984.0| train_rmse: 138017.22506| valid_rmse: 140839.76411|  0:00:46s\n",
      "epoch 111| loss: 22070255616.0| train_rmse: 135182.45617| valid_rmse: 137733.26028|  0:00:46s\n",
      "epoch 112| loss: 21809049600.0| train_rmse: 134567.69094| valid_rmse: 137050.04402|  0:00:47s\n",
      "epoch 113| loss: 21527474176.0| train_rmse: 133035.93256| valid_rmse: 135669.35413|  0:00:48s\n",
      "epoch 114| loss: 21255698432.0| train_rmse: 132386.32546| valid_rmse: 135101.36894|  0:00:48s\n",
      "epoch 115| loss: 20991262720.0| train_rmse: 128698.9437| valid_rmse: 131369.45743|  0:00:48s\n",
      "epoch 116| loss: 20720011264.0| train_rmse: 127267.38577| valid_rmse: 130082.86643|  0:00:49s\n",
      "epoch 117| loss: 20428587008.0| train_rmse: 129086.47742| valid_rmse: 132001.97677|  0:00:49s\n",
      "epoch 118| loss: 20167632896.0| train_rmse: 130702.35948| valid_rmse: 133658.39024|  0:00:50s\n",
      "epoch 119| loss: 19928238080.0| train_rmse: 128377.13841| valid_rmse: 131491.5271|  0:00:50s\n",
      "epoch 120| loss: 19649734656.0| train_rmse: 126620.87719| valid_rmse: 129708.48394|  0:00:51s\n",
      "epoch 121| loss: 19387807744.0| train_rmse: 128721.77267| valid_rmse: 131734.80315|  0:00:51s\n",
      "epoch 122| loss: 19135963136.0| train_rmse: 132889.09364| valid_rmse: 135740.07243|  0:00:52s\n",
      "epoch 123| loss: 18824251392.0| train_rmse: 133955.51065| valid_rmse: 136652.73824|  0:00:52s\n",
      "epoch 124| loss: 18610726912.0| train_rmse: 129277.37212| valid_rmse: 131932.97256|  0:00:53s\n",
      "epoch 125| loss: 18378463232.0| train_rmse: 122958.21258| valid_rmse: 125292.9913|  0:00:53s\n",
      "epoch 126| loss: 18116870144.0| train_rmse: 121648.17951| valid_rmse: 123672.89988|  0:00:54s\n",
      "epoch 127| loss: 17923215360.0| train_rmse: 125528.41782| valid_rmse: 127333.43642|  0:00:54s\n",
      "epoch 128| loss: 17654001664.0| train_rmse: 131562.60868| valid_rmse: 133324.65834|  0:00:54s\n",
      "epoch 129| loss: 17463390208.0| train_rmse: 132670.56806| valid_rmse: 134633.80763|  0:00:55s\n",
      "epoch 130| loss: 17221707776.0| train_rmse: 129526.81044| valid_rmse: 131518.49501|  0:00:55s\n",
      "epoch 131| loss: 16973850624.0| train_rmse: 126692.84489| valid_rmse: 128520.2828|  0:00:56s\n",
      "epoch 132| loss: 16749543424.0| train_rmse: 127444.08183| valid_rmse: 129267.3331|  0:00:56s\n",
      "epoch 133| loss: 16552444928.0| train_rmse: 129433.85887| valid_rmse: 131393.34806|  0:00:57s\n",
      "epoch 134| loss: 16318085120.0| train_rmse: 127154.85456| valid_rmse: 129184.43079|  0:00:57s\n",
      "epoch 135| loss: 16075111424.0| train_rmse: 124462.69435| valid_rmse: 126551.03428|  0:00:57s\n",
      "epoch 136| loss: 15874510848.0| train_rmse: 119942.97381| valid_rmse: 121811.05028|  0:00:58s\n",
      "epoch 137| loss: 15666215936.0| train_rmse: 119688.35262| valid_rmse: 121678.15375|  0:00:58s\n",
      "epoch 138| loss: 15429204992.0| train_rmse: 115438.03251| valid_rmse: 117054.80825|  0:00:59s\n",
      "epoch 139| loss: 15284030464.0| train_rmse: 109082.3041| valid_rmse: 110215.6977|  0:00:59s\n",
      "epoch 140| loss: 15101970432.0| train_rmse: 110217.79591| valid_rmse: 111899.38424|  0:01:00s\n",
      "epoch 141| loss: 14976697344.0| train_rmse: 114847.57972| valid_rmse: 116767.65611|  0:01:00s\n",
      "epoch 142| loss: 14786028544.0| train_rmse: 121365.86074| valid_rmse: 123545.29925|  0:01:01s\n",
      "epoch 143| loss: 14570408960.0| train_rmse: 124615.22475| valid_rmse: 127018.39714|  0:01:01s\n",
      "epoch 144| loss: 14374121472.0| train_rmse: 127291.67514| valid_rmse: 128924.58647|  0:01:01s\n",
      "epoch 145| loss: 14087712768.0| train_rmse: 131461.72747| valid_rmse: 132535.14|  0:01:02s\n",
      "epoch 146| loss: 14142434304.0| train_rmse: 122925.89043| valid_rmse: 124200.34525|  0:01:02s\n",
      "epoch 147| loss: 14134762496.0| train_rmse: 108772.39995| valid_rmse: 110546.00624|  0:01:02s\n",
      "epoch 148| loss: 13796713472.0| train_rmse: 97410.61714| valid_rmse: 99946.86799|  0:01:03s\n",
      "epoch 149| loss: 13692735488.0| train_rmse: 93838.08732| valid_rmse: 96562.53038|  0:01:03s\n",
      "epoch 150| loss: 13411789824.0| train_rmse: 98208.86426| valid_rmse: 101338.72608|  0:01:04s\n",
      "epoch 151| loss: 13198331904.0| train_rmse: 106783.5997| valid_rmse: 109504.25741|  0:01:04s\n",
      "epoch 152| loss: 13014945792.0| train_rmse: 111332.73881| valid_rmse: 113897.57447|  0:01:04s\n",
      "epoch 153| loss: 12790072320.0| train_rmse: 115042.86309| valid_rmse: 117389.76273|  0:01:05s\n",
      "epoch 154| loss: 12686441472.0| train_rmse: 110284.78999| valid_rmse: 112747.13476|  0:01:05s\n",
      "epoch 155| loss: 12529912832.0| train_rmse: 107062.84903| valid_rmse: 109991.12862|  0:01:06s\n",
      "epoch 156| loss: 12454993920.0| train_rmse: 98371.66716| valid_rmse: 102100.95831|  0:01:06s\n",
      "epoch 157| loss: 12211444736.0| train_rmse: 93497.71238| valid_rmse: 97530.46109|  0:01:06s\n",
      "epoch 158| loss: 12035706880.0| train_rmse: 89465.93445| valid_rmse: 92729.76978|  0:01:07s\n",
      "epoch 159| loss: 11877160960.0| train_rmse: 85088.82785| valid_rmse: 89075.4235|  0:01:07s\n",
      "epoch 160| loss: 11771781120.0| train_rmse: 85196.74283| valid_rmse: 89348.11195|  0:01:08s\n",
      "epoch 161| loss: 11583327232.0| train_rmse: 86471.39763| valid_rmse: 90517.47283|  0:01:08s\n",
      "epoch 162| loss: 11454330880.0| train_rmse: 89784.93464| valid_rmse: 93792.85303|  0:01:09s\n",
      "epoch 163| loss: 11324361728.0| train_rmse: 93657.38223| valid_rmse: 97452.03479|  0:01:09s\n",
      "epoch 164| loss: 11153584128.0| train_rmse: 98707.7924| valid_rmse: 102043.95697|  0:01:09s\n",
      "epoch 165| loss: 10984892416.0| train_rmse: 104018.6643| valid_rmse: 106298.54482|  0:01:10s\n",
      "epoch 166| loss: 10819191808.0| train_rmse: 108974.21421| valid_rmse: 110273.77758|  0:01:10s\n",
      "epoch 167| loss: 10812580864.0| train_rmse: 111392.43486| valid_rmse: 111963.93557|  0:01:11s\n",
      "epoch 168| loss: 10812304384.0| train_rmse: 108422.0427| valid_rmse: 109039.47781|  0:01:11s\n",
      "epoch 169| loss: 10623369216.0| train_rmse: 100522.36858| valid_rmse: 101667.01022|  0:01:12s\n",
      "epoch 170| loss: 10455860224.0| train_rmse: 93584.51292| valid_rmse: 94737.29667|  0:01:12s\n",
      "epoch 171| loss: 10302219264.0| train_rmse: 88271.29462| valid_rmse: 90084.77509|  0:01:12s\n",
      "epoch 172| loss: 10275547136.0| train_rmse: 86163.1199| valid_rmse: 88185.49359|  0:01:13s\n",
      "epoch 173| loss: 10070064128.0| train_rmse: 90279.90764| valid_rmse: 92528.66263|  0:01:13s\n",
      "epoch 174| loss: 9849604096.0| train_rmse: 97343.08714| valid_rmse: 99817.97134|  0:01:14s\n",
      "epoch 175| loss: 9740522496.0| train_rmse: 96928.62387| valid_rmse: 99724.81658|  0:01:14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 176| loss: 9609788416.0| train_rmse: 89011.48058| valid_rmse: 91681.84273|  0:01:15s\n",
      "epoch 177| loss: 9370310656.0| train_rmse: 78827.72623| valid_rmse: 81981.23025|  0:01:15s\n",
      "epoch 178| loss: 9236921344.0| train_rmse: 74806.87276| valid_rmse: 77326.3935|  0:01:15s\n",
      "epoch 179| loss: 9025915904.0| train_rmse: 74476.60723| valid_rmse: 76460.09492|  0:01:16s\n",
      "epoch 180| loss: 8914034688.0| train_rmse: 77070.42575| valid_rmse: 79070.15883|  0:01:16s\n",
      "epoch 181| loss: 8842962944.0| train_rmse: 77682.76211| valid_rmse: 79605.03034|  0:01:17s\n",
      "epoch 182| loss: 8720616448.0| train_rmse: 76001.68904| valid_rmse: 78172.51418|  0:01:17s\n",
      "epoch 183| loss: 8575387136.0| train_rmse: 72242.73407| valid_rmse: 74710.25826|  0:01:17s\n",
      "epoch 184| loss: 8473285632.0| train_rmse: 66098.08515| valid_rmse: 69471.10842|  0:01:18s\n",
      "epoch 185| loss: 8267112448.0| train_rmse: 59114.0593| valid_rmse: 62791.82119|  0:01:18s\n",
      "epoch 186| loss: 8256608768.0| train_rmse: 57228.82938| valid_rmse: 61717.58727|  0:01:19s\n",
      "epoch 187| loss: 8100306944.0| train_rmse: 57914.47907| valid_rmse: 62615.64469|  0:01:20s\n",
      "epoch 188| loss: 7990034432.0| train_rmse: 60892.52016| valid_rmse: 65443.48035|  0:01:20s\n",
      "epoch 189| loss: 7857882624.0| train_rmse: 67668.15087| valid_rmse: 71999.29802|  0:01:21s\n",
      "epoch 190| loss: 7735823872.0| train_rmse: 76685.01081| valid_rmse: 80575.34342|  0:01:22s\n",
      "epoch 191| loss: 7560805888.0| train_rmse: 84870.09838| valid_rmse: 88074.5585|  0:01:23s\n",
      "epoch 192| loss: 7414427648.0| train_rmse: 82507.2906| valid_rmse: 85698.27107|  0:01:23s\n",
      "epoch 193| loss: 7335907328.0| train_rmse: 79991.27326| valid_rmse: 83007.75121|  0:01:24s\n",
      "epoch 194| loss: 7227873792.0| train_rmse: 74504.50057| valid_rmse: 77719.63962|  0:01:25s\n",
      "epoch 195| loss: 7009620480.0| train_rmse: 66796.09946| valid_rmse: 70500.75821|  0:01:26s\n",
      "epoch 196| loss: 6960125952.0| train_rmse: 62586.49565| valid_rmse: 66473.31738|  0:01:26s\n",
      "epoch 197| loss: 6880503808.0| train_rmse: 63094.04135| valid_rmse: 67328.92398|  0:01:27s\n",
      "epoch 198| loss: 6711904768.0| train_rmse: 66402.9728| valid_rmse: 69985.49043|  0:01:27s\n",
      "epoch 199| loss: 6712579072.0| train_rmse: 67032.94302| valid_rmse: 70504.76475|  0:01:28s\n",
      "epoch 200| loss: 6596323328.0| train_rmse: 64272.42704| valid_rmse: 68074.26411|  0:01:28s\n",
      "epoch 201| loss: 6467558400.0| train_rmse: 60580.57323| valid_rmse: 64649.59673|  0:01:28s\n",
      "epoch 202| loss: 6346130944.0| train_rmse: 56931.3902| valid_rmse: 60915.61079|  0:01:29s\n",
      "epoch 203| loss: 6218817536.0| train_rmse: 56793.82817| valid_rmse: 60617.29678|  0:01:30s\n",
      "epoch 204| loss: 6127418368.0| train_rmse: 56800.66409| valid_rmse: 60391.72829|  0:01:31s\n",
      "epoch 205| loss: 6008562688.0| train_rmse: 63845.17497| valid_rmse: 67383.29145|  0:01:31s\n",
      "epoch 206| loss: 5839577600.0| train_rmse: 72744.51903| valid_rmse: 75749.94401|  0:01:32s\n",
      "epoch 207| loss: 5726583296.0| train_rmse: 77470.14404| valid_rmse: 79680.3271|  0:01:33s\n",
      "epoch 208| loss: 5612946944.0| train_rmse: 77448.5029| valid_rmse: 79193.68007|  0:01:34s\n",
      "epoch 209| loss: 5518048768.0| train_rmse: 74324.56648| valid_rmse: 76356.6681|  0:01:34s\n",
      "epoch 210| loss: 5428038656.0| train_rmse: 67579.41152| valid_rmse: 70038.94198|  0:01:35s\n",
      "epoch 211| loss: 5294367232.0| train_rmse: 65409.51322| valid_rmse: 68154.58427|  0:01:36s\n",
      "epoch 212| loss: 5177895424.0| train_rmse: 69196.51777| valid_rmse: 71719.0512|  0:01:36s\n",
      "epoch 213| loss: 5098659840.0| train_rmse: 78703.26807| valid_rmse: 80902.57515|  0:01:37s\n",
      "epoch 214| loss: 5041884672.0| train_rmse: 81799.83925| valid_rmse: 83535.18139|  0:01:38s\n",
      "epoch 215| loss: 5001523200.0| train_rmse: 93901.98874| valid_rmse: 95453.21111|  0:01:38s\n",
      "epoch 216| loss: 5064500224.0| train_rmse: 94690.34695| valid_rmse: 96332.40499|  0:01:39s\n",
      "epoch 217| loss: 4945915392.0| train_rmse: 83669.06457| valid_rmse: 86187.40296|  0:01:39s\n",
      "epoch 218| loss: 4831022592.0| train_rmse: 69425.70213| valid_rmse: 73361.30126|  0:01:40s\n",
      "epoch 219| loss: 4651976704.0| train_rmse: 63505.0299| valid_rmse: 67726.36008|  0:01:40s\n",
      "epoch 220| loss: 4582125056.0| train_rmse: 63790.5407| valid_rmse: 67745.67918|  0:01:41s\n",
      "epoch 221| loss: 4523901440.0| train_rmse: 69275.07808| valid_rmse: 72562.82723|  0:01:41s\n",
      "epoch 222| loss: 4387930624.0| train_rmse: 82991.59582| valid_rmse: 86472.80155|  0:01:42s\n",
      "epoch 223| loss: 4497745920.0| train_rmse: 85969.58821| valid_rmse: 87730.42503|  0:01:42s\n",
      "epoch 224| loss: 4499057152.0| train_rmse: 77034.05339| valid_rmse: 79813.10446|  0:01:43s\n",
      "epoch 225| loss: 4459971072.0| train_rmse: 60039.72688| valid_rmse: 63713.00203|  0:01:44s\n",
      "epoch 226| loss: 4256402432.0| train_rmse: 53828.85447| valid_rmse: 58504.23827|  0:01:44s\n",
      "epoch 227| loss: 4340708352.0| train_rmse: 45853.06286| valid_rmse: 51648.87157|  0:01:45s\n",
      "epoch 228| loss: 4022312448.0| train_rmse: 45925.16528| valid_rmse: 51327.97746|  0:01:45s\n",
      "epoch 229| loss: 3961295872.0| train_rmse: 43003.63857| valid_rmse: 48909.46901|  0:01:46s\n",
      "epoch 230| loss: 3922315776.0| train_rmse: 43951.01416| valid_rmse: 49790.41955|  0:01:46s\n",
      "epoch 231| loss: 3931493888.0| train_rmse: 46839.59192| valid_rmse: 52552.83982|  0:01:47s\n",
      "epoch 232| loss: 3760188672.0| train_rmse: 52730.83447| valid_rmse: 57417.11541|  0:01:47s\n",
      "epoch 233| loss: 3633603072.0| train_rmse: 62076.81841| valid_rmse: 65752.52817|  0:01:47s\n",
      "epoch 234| loss: 3587557632.0| train_rmse: 76726.28147| valid_rmse: 80027.07943|  0:01:48s\n",
      "epoch 235| loss: 3595129600.0| train_rmse: 86099.92986| valid_rmse: 88282.52135|  0:01:49s\n",
      "epoch 236| loss: 3865277184.0| train_rmse: 87867.55577| valid_rmse: 88804.21766|  0:01:49s\n",
      "epoch 237| loss: 3942333696.0| train_rmse: 83977.54539| valid_rmse: 85034.58543|  0:01:50s\n",
      "epoch 238| loss: 3812495616.0| train_rmse: 84451.11575| valid_rmse: 84604.76592|  0:01:50s\n",
      "epoch 239| loss: 3811076352.0| train_rmse: 80422.23458| valid_rmse: 81686.01632|  0:01:51s\n",
      "epoch 240| loss: 3718570240.0| train_rmse: 72396.40693| valid_rmse: 74889.77807|  0:01:51s\n",
      "epoch 241| loss: 3608330240.0| train_rmse: 69163.38599| valid_rmse: 72134.51195|  0:01:52s\n",
      "epoch 242| loss: 3552940800.0| train_rmse: 68632.28466| valid_rmse: 71141.53969|  0:01:52s\n",
      "epoch 243| loss: 3600101120.0| train_rmse: 62298.01131| valid_rmse: 66158.30492|  0:01:52s\n",
      "epoch 244| loss: 3430327552.0| train_rmse: 61401.77606| valid_rmse: 65127.45169|  0:01:53s\n",
      "epoch 245| loss: 3416598528.0| train_rmse: 64817.21737| valid_rmse: 67543.70598|  0:01:53s\n",
      "epoch 246| loss: 3465850368.0| train_rmse: 70583.89392| valid_rmse: 71790.05372|  0:01:54s\n",
      "epoch 247| loss: 3597364992.0| train_rmse: 70035.23887| valid_rmse: 71347.89409|  0:01:54s\n",
      "epoch 248| loss: 3551309056.0| train_rmse: 74477.43404| valid_rmse: 74255.23403|  0:01:55s\n",
      "epoch 249| loss: 3488449280.0| train_rmse: 74973.03644| valid_rmse: 75221.72689|  0:01:55s\n",
      "epoch 250| loss: 3398855424.0| train_rmse: 71900.13386| valid_rmse: 72368.33648|  0:01:55s\n",
      "epoch 251| loss: 3491039232.0| train_rmse: 67840.27138| valid_rmse: 69845.24777|  0:01:56s\n",
      "epoch 252| loss: 3304940800.0| train_rmse: 47338.82856| valid_rmse: 51792.21497|  0:01:56s\n",
      "epoch 253| loss: 3258884096.0| train_rmse: 46557.29323| valid_rmse: 50870.55044|  0:01:56s\n",
      "epoch 254| loss: 3193132544.0| train_rmse: 43809.5897| valid_rmse: 48422.61091|  0:01:57s\n",
      "epoch 255| loss: 3256356096.0| train_rmse: 44156.73377| valid_rmse: 48678.02563|  0:01:57s\n",
      "epoch 256| loss: 3197217280.0| train_rmse: 43273.47644| valid_rmse: 48168.56247|  0:01:58s\n",
      "epoch 257| loss: 3344775680.0| train_rmse: 41058.38028| valid_rmse: 46243.81734|  0:01:58s\n",
      "epoch 258| loss: 3151341312.0| train_rmse: 38671.26443| valid_rmse: 44014.39351|  0:01:59s\n",
      "epoch 259| loss: 2940922624.0| train_rmse: 40821.83991| valid_rmse: 45302.49343|  0:01:59s\n",
      "epoch 260| loss: 2942601216.0| train_rmse: 40439.5708| valid_rmse: 44971.23029|  0:01:59s\n",
      "epoch 261| loss: 2876055040.0| train_rmse: 40737.05957| valid_rmse: 45019.8943|  0:02:00s\n",
      "epoch 262| loss: 2851324416.0| train_rmse: 41359.95602| valid_rmse: 45795.06912|  0:02:00s\n",
      "epoch 263| loss: 2743892224.0| train_rmse: 44360.12138| valid_rmse: 48059.27099|  0:02:01s\n",
      "epoch 264| loss: 2678949888.0| train_rmse: 50934.36342| valid_rmse: 52872.19045|  0:02:01s\n",
      "epoch 265| loss: 2643791360.0| train_rmse: 55531.39427| valid_rmse: 56302.27376|  0:02:02s\n",
      "epoch 266| loss: 2558521344.0| train_rmse: 57275.52413| valid_rmse: 58983.94605|  0:02:02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 267| loss: 2491787520.0| train_rmse: 56447.02094| valid_rmse: 58457.82496|  0:02:02s\n",
      "epoch 268| loss: 2373321984.0| train_rmse: 59681.50713| valid_rmse: 61780.26454|  0:02:03s\n",
      "epoch 269| loss: 2366068224.0| train_rmse: 65358.88223| valid_rmse: 66635.27838|  0:02:03s\n",
      "epoch 270| loss: 2463415808.0| train_rmse: 73280.07111| valid_rmse: 74420.49694|  0:02:03s\n",
      "epoch 271| loss: 2651817472.0| train_rmse: 77291.62627| valid_rmse: 78341.85513|  0:02:04s\n",
      "epoch 272| loss: 2860750592.0| train_rmse: 76241.66085| valid_rmse: 77104.98568|  0:02:04s\n",
      "epoch 273| loss: 2798136064.0| train_rmse: 71146.21484| valid_rmse: 72324.49409|  0:02:05s\n",
      "epoch 274| loss: 2741483264.0| train_rmse: 61420.11641| valid_rmse: 62870.14092|  0:02:05s\n",
      "epoch 275| loss: 2511270144.0| train_rmse: 38895.0705| valid_rmse: 42719.47162|  0:02:06s\n",
      "epoch 276| loss: 2313064704.0| train_rmse: 36803.8769| valid_rmse: 41628.87309|  0:02:06s\n",
      "epoch 277| loss: 2127762304.0| train_rmse: 49310.68861| valid_rmse: 52728.51862|  0:02:07s\n",
      "epoch 278| loss: 2112506112.0| train_rmse: 53945.03622| valid_rmse: 56890.35752|  0:02:07s\n",
      "epoch 279| loss: 2149858048.0| train_rmse: 45730.76463| valid_rmse: 49753.93935|  0:02:07s\n",
      "epoch 280| loss: 2122756480.0| train_rmse: 40926.3258| valid_rmse: 45987.88105|  0:02:08s\n",
      "epoch 281| loss: 2024560768.0| train_rmse: 35707.75092| valid_rmse: 41977.10216|  0:02:08s\n",
      "epoch 282| loss: 1952435456.0| train_rmse: 37995.51179| valid_rmse: 44216.67886|  0:02:08s\n",
      "epoch 283| loss: 1942253056.0| train_rmse: 48431.31004| valid_rmse: 53554.60375|  0:02:09s\n",
      "epoch 284| loss: 1866122496.0| train_rmse: 58222.55| valid_rmse: 61927.1912|  0:02:09s\n",
      "epoch 285| loss: 1778359680.0| train_rmse: 61783.53156| valid_rmse: 65378.57498|  0:02:09s\n",
      "epoch 286| loss: 1698599424.0| train_rmse: 58583.5212| valid_rmse: 62769.59473|  0:02:10s\n",
      "epoch 287| loss: 1707911936.0| train_rmse: 52040.40124| valid_rmse: 57332.6398|  0:02:10s\n",
      "epoch 288| loss: 1699910656.0| train_rmse: 49686.19877| valid_rmse: 55328.54155|  0:02:10s\n",
      "epoch 289| loss: 1733165056.0| train_rmse: 44728.28171| valid_rmse: 50861.60052|  0:02:11s\n",
      "epoch 290| loss: 1654728704.0| train_rmse: 45582.93823| valid_rmse: 51492.05826|  0:02:11s\n",
      "epoch 291| loss: 1585395328.0| train_rmse: 54990.91774| valid_rmse: 59630.57881|  0:02:12s\n",
      "epoch 292| loss: 1476546560.0| train_rmse: 57349.31119| valid_rmse: 61282.80701|  0:02:12s\n",
      "epoch 293| loss: 1580439040.0| train_rmse: 67742.2779| valid_rmse: 70867.84058|  0:02:12s\n",
      "epoch 294| loss: 1933720576.0| train_rmse: 67099.39493| valid_rmse: 69926.66532|  0:02:13s\n",
      "epoch 295| loss: 2248090368.0| train_rmse: 71472.65967| valid_rmse: 76011.61294|  0:02:13s\n",
      "epoch 296| loss: 2792235264.0| train_rmse: 67606.97743| valid_rmse: 71516.79091|  0:02:13s\n",
      "epoch 297| loss: 2796865024.0| train_rmse: 56157.76239| valid_rmse: 60368.03063|  0:02:14s\n",
      "epoch 298| loss: 2610016512.0| train_rmse: 39026.23303| valid_rmse: 46211.1165|  0:02:14s\n",
      "epoch 299| loss: 2118915968.0| train_rmse: 49157.82911| valid_rmse: 53900.60318|  0:02:14s\n",
      "epoch 300| loss: 1815695360.0| train_rmse: 59632.54188| valid_rmse: 62256.13105|  0:02:15s\n",
      "epoch 301| loss: 1738879104.0| train_rmse: 75176.34183| valid_rmse: 75379.2647|  0:02:15s\n",
      "epoch 302| loss: 1680105856.0| train_rmse: 70953.68726| valid_rmse: 70969.15703|  0:02:15s\n",
      "epoch 303| loss: 1631406208.0| train_rmse: 57704.2614| valid_rmse: 58225.6777|  0:02:16s\n",
      "epoch 304| loss: 1537788544.0| train_rmse: 41949.04582| valid_rmse: 44091.12967|  0:02:16s\n",
      "epoch 305| loss: 1522587776.0| train_rmse: 35253.55589| valid_rmse: 40689.51231|  0:02:17s\n",
      "epoch 306| loss: 1539699584.0| train_rmse: 43869.66685| valid_rmse: 49679.64218|  0:02:17s\n",
      "epoch 307| loss: 1551942528.0| train_rmse: 46057.55199| valid_rmse: 51561.05428|  0:02:17s\n",
      "epoch 308| loss: 1582792832.0| train_rmse: 41102.9212| valid_rmse: 47353.19355|  0:02:18s\n",
      "epoch 309| loss: 1469816320.0| train_rmse: 44417.56662| valid_rmse: 50388.41291|  0:02:18s\n",
      "epoch 310| loss: 1472319360.0| train_rmse: 52175.29797| valid_rmse: 60151.63572|  0:02:18s\n",
      "epoch 311| loss: 1369657472.0| train_rmse: 60342.04876| valid_rmse: 67622.15532|  0:02:19s\n",
      "epoch 312| loss: 1648716160.0| train_rmse: 59177.95762| valid_rmse: 66850.15427|  0:02:20s\n",
      "epoch 313| loss: 1665802624.0| train_rmse: 50358.90476| valid_rmse: 58554.36868|  0:02:20s\n",
      "epoch 314| loss: 1492992640.0| train_rmse: 50181.63569| valid_rmse: 58771.39132|  0:02:20s\n",
      "epoch 315| loss: 1558168320.0| train_rmse: 39672.89186| valid_rmse: 49258.13951|  0:02:21s\n",
      "epoch 316| loss: 1345381632.0| train_rmse: 33442.21086| valid_rmse: 43201.79424|  0:02:21s\n",
      "epoch 317| loss: 1321021952.0| train_rmse: 34628.61219| valid_rmse: 43651.76203|  0:02:22s\n",
      "epoch 318| loss: 1316873344.0| train_rmse: 43325.97864| valid_rmse: 49423.10643|  0:02:22s\n",
      "epoch 319| loss: 1317411328.0| train_rmse: 43612.6767| valid_rmse: 49245.33485|  0:02:23s\n",
      "epoch 320| loss: 1349864192.0| train_rmse: 37174.49942| valid_rmse: 44750.49298|  0:02:23s\n",
      "epoch 321| loss: 1183674624.0| train_rmse: 31336.76568| valid_rmse: 41350.79488|  0:02:23s\n",
      "epoch 322| loss: 1305281024.0| train_rmse: 31718.58156| valid_rmse: 40940.52167|  0:02:24s\n",
      "epoch 323| loss: 1349500416.0| train_rmse: 33158.57932| valid_rmse: 41851.25734|  0:02:24s\n",
      "epoch 324| loss: 1215500800.0| train_rmse: 30493.29583| valid_rmse: 40157.79708|  0:02:25s\n",
      "epoch 325| loss: 1292913664.0| train_rmse: 31009.34147| valid_rmse: 40462.19889|  0:02:25s\n",
      "epoch 326| loss: 1162985728.0| train_rmse: 33782.0323| valid_rmse: 41652.14362|  0:02:25s\n",
      "epoch 327| loss: 1061327360.0| train_rmse: 31757.76649| valid_rmse: 40124.90976|  0:02:26s\n",
      "epoch 328| loss: 1134303616.0| train_rmse: 29813.65538| valid_rmse: 39489.5961|  0:02:26s\n",
      "epoch 329| loss: 1095088512.0| train_rmse: 32515.42785| valid_rmse: 42372.54323|  0:02:27s\n",
      "epoch 330| loss: 1065219904.0| train_rmse: 38929.2487| valid_rmse: 47688.38824|  0:02:27s\n",
      "epoch 331| loss: 1058177088.0| train_rmse: 34845.06529| valid_rmse: 44261.52306|  0:02:27s\n",
      "epoch 332| loss: 1065218048.0| train_rmse: 28587.49336| valid_rmse: 39285.78061|  0:02:28s\n",
      "epoch 333| loss: 1066022592.0| train_rmse: 37182.09087| valid_rmse: 44258.03718|  0:02:28s\n",
      "epoch 334| loss: 1042831296.0| train_rmse: 39816.03592| valid_rmse: 45967.28059|  0:02:29s\n",
      "epoch 335| loss: 1019673408.0| train_rmse: 33257.20365| valid_rmse: 40800.45245|  0:02:29s\n",
      "epoch 336| loss: 993930432.0| train_rmse: 35215.35441| valid_rmse: 41744.13947|  0:02:29s\n",
      "epoch 337| loss: 1094041472.0| train_rmse: 32673.25426| valid_rmse: 39795.43666|  0:02:30s\n",
      "epoch 338| loss: 1199409792.0| train_rmse: 32983.88452| valid_rmse: 40331.71113|  0:02:31s\n",
      "epoch 339| loss: 1064178816.0| train_rmse: 30438.26457| valid_rmse: 39484.43956|  0:02:31s\n",
      "epoch 340| loss: 1005445504.0| train_rmse: 43049.5125| valid_rmse: 49338.19164|  0:02:32s\n",
      "epoch 341| loss: 1101737728.0| train_rmse: 65213.13171| valid_rmse: 71059.01812|  0:02:32s\n",
      "epoch 342| loss: 1822382848.0| train_rmse: 72185.4125| valid_rmse: 80513.30798|  0:02:32s\n",
      "epoch 343| loss: 2497375232.0| train_rmse: 72286.85582| valid_rmse: 80481.17416|  0:02:33s\n",
      "epoch 344| loss: 2662746880.0| train_rmse: 69144.36115| valid_rmse: 75107.66738|  0:02:33s\n",
      "epoch 345| loss: 2714019072.0| train_rmse: 72205.12158| valid_rmse: 79768.90463|  0:02:34s\n",
      "epoch 346| loss: 2644955648.0| train_rmse: 70373.72418| valid_rmse: 76815.70158|  0:02:34s\n",
      "epoch 347| loss: 2422046720.0| train_rmse: 62569.89105| valid_rmse: 67811.33246|  0:02:34s\n",
      "epoch 348| loss: 1941974656.0| train_rmse: 47919.63881| valid_rmse: 54229.51078|  0:02:35s\n",
      "epoch 349| loss: 1541043584.0| train_rmse: 33252.8624| valid_rmse: 41378.85657|  0:02:35s\n",
      "epoch 350| loss: 1328668032.0| train_rmse: 52558.43529| valid_rmse: 56190.41119|  0:02:36s\n",
      "epoch 351| loss: 1201040128.0| train_rmse: 91565.73972| valid_rmse: 91587.52403|  0:02:36s\n",
      "epoch 352| loss: 1243311104.0| train_rmse: 104154.71404| valid_rmse: 103372.89054|  0:02:37s\n",
      "epoch 353| loss: 1267270656.0| train_rmse: 96099.68855| valid_rmse: 95713.47174|  0:02:37s\n",
      "epoch 354| loss: 1326353920.0| train_rmse: 77963.72903| valid_rmse: 78531.9459|  0:02:38s\n",
      "epoch 355| loss: 1256504448.0| train_rmse: 55776.69452| valid_rmse: 58314.63507|  0:02:38s\n",
      "epoch 356| loss: 1157801344.0| train_rmse: 37046.05291| valid_rmse: 42709.83644|  0:02:39s\n",
      "epoch 357| loss: 1126419328.0| train_rmse: 34357.83057| valid_rmse: 41684.51187|  0:02:39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 358| loss: 1196623488.0| train_rmse: 45619.92523| valid_rmse: 52227.1468|  0:02:39s\n",
      "epoch 359| loss: 1214882304.0| train_rmse: 50606.15293| valid_rmse: 56778.97504|  0:02:40s\n",
      "epoch 360| loss: 1171945728.0| train_rmse: 49667.51382| valid_rmse: 56200.32737|  0:02:40s\n",
      "epoch 361| loss: 1016335360.0| train_rmse: 39710.116| valid_rmse: 47725.62037|  0:02:41s\n",
      "epoch 362| loss: 917497536.0| train_rmse: 37607.78375| valid_rmse: 46099.36639|  0:02:41s\n",
      "epoch 363| loss: 921652800.0| train_rmse: 30268.10812| valid_rmse: 40024.82118|  0:02:42s\n",
      "epoch 364| loss: 831636736.0| train_rmse: 30504.19166| valid_rmse: 38676.17659|  0:02:42s\n",
      "epoch 365| loss: 947042368.0| train_rmse: 31440.94027| valid_rmse: 39394.57957|  0:02:43s\n",
      "epoch 366| loss: 950457728.0| train_rmse: 30146.96527| valid_rmse: 39201.83897|  0:02:43s\n",
      "epoch 367| loss: 933328128.0| train_rmse: 31288.61647| valid_rmse: 41236.27693|  0:02:44s\n",
      "epoch 368| loss: 852032000.0| train_rmse: 35865.13156| valid_rmse: 43994.11441|  0:02:45s\n",
      "epoch 369| loss: 795508544.0| train_rmse: 49974.81719| valid_rmse: 56627.23027|  0:02:45s\n",
      "epoch 370| loss: 922384960.0| train_rmse: 59944.73692| valid_rmse: 63492.20787|  0:02:46s\n",
      "epoch 371| loss: 1214772736.0| train_rmse: 70690.79787| valid_rmse: 74158.53739|  0:02:46s\n",
      "epoch 372| loss: 1855852928.0| train_rmse: 73064.76669| valid_rmse: 75083.42901|  0:02:47s\n",
      "epoch 373| loss: 2316339712.0| train_rmse: 70932.10126| valid_rmse: 72481.23557|  0:02:48s\n",
      "epoch 374| loss: 2506576128.0| train_rmse: 65376.67273| valid_rmse: 68177.93327|  0:02:48s\n",
      "epoch 375| loss: 2263853824.0| train_rmse: 61835.6405| valid_rmse: 66199.03951|  0:02:49s\n",
      "epoch 376| loss: 2072635136.0| train_rmse: 52901.17943| valid_rmse: 59139.93992|  0:02:49s\n",
      "epoch 377| loss: 1892790784.0| train_rmse: 47135.30247| valid_rmse: 55471.96049|  0:02:50s\n",
      "epoch 378| loss: 1657382656.0| train_rmse: 39606.89977| valid_rmse: 47898.55965|  0:02:50s\n",
      "epoch 379| loss: 1403519232.0| train_rmse: 37947.9807| valid_rmse: 45483.58487|  0:02:51s\n",
      "epoch 380| loss: 1516342016.0| train_rmse: 39525.40165| valid_rmse: 46573.33316|  0:02:51s\n",
      "epoch 381| loss: 1551863552.0| train_rmse: 39022.47196| valid_rmse: 45695.27571|  0:02:52s\n",
      "epoch 382| loss: 1532553344.0| train_rmse: 39012.99302| valid_rmse: 44621.33517|  0:02:52s\n",
      "epoch 383| loss: 1367950208.0| train_rmse: 42704.83653| valid_rmse: 47487.60794|  0:02:53s\n",
      "epoch 384| loss: 1308871936.0| train_rmse: 55519.99686| valid_rmse: 60356.58304|  0:02:53s\n",
      "epoch 385| loss: 1233212544.0| train_rmse: 74508.08656| valid_rmse: 78538.58802|  0:02:54s\n",
      "epoch 386| loss: 1056895360.0| train_rmse: 90266.51015| valid_rmse: 92988.80194|  0:02:54s\n",
      "epoch 387| loss: 984260096.0| train_rmse: 90831.54693| valid_rmse: 94292.83305|  0:02:55s\n",
      "epoch 388| loss: 978569216.0| train_rmse: 82242.51154| valid_rmse: 86468.75729|  0:02:55s\n",
      "epoch 389| loss: 995748736.0| train_rmse: 66586.89952| valid_rmse: 72878.75826|  0:02:56s\n",
      "epoch 390| loss: 957655488.0| train_rmse: 41465.47609| valid_rmse: 51042.50668|  0:02:56s\n",
      "epoch 391| loss: 965134272.0| train_rmse: 36384.95828| valid_rmse: 47342.90656|  0:02:57s\n",
      "epoch 392| loss: 894713600.0| train_rmse: 35460.86405| valid_rmse: 46671.99372|  0:02:58s\n",
      "epoch 393| loss: 927217088.0| train_rmse: 39540.42782| valid_rmse: 49076.9008|  0:02:58s\n",
      "epoch 394| loss: 947132032.0| train_rmse: 40048.47232| valid_rmse: 49306.6909|  0:02:59s\n",
      "epoch 395| loss: 959857472.0| train_rmse: 47245.81149| valid_rmse: 54236.9306|  0:02:59s\n",
      "epoch 396| loss: 934981120.0| train_rmse: 63807.14946| valid_rmse: 66899.96749|  0:03:00s\n",
      "epoch 397| loss: 888529408.0| train_rmse: 65918.02236| valid_rmse: 68495.74435|  0:03:00s\n",
      "epoch 398| loss: 895379712.0| train_rmse: 53821.47329| valid_rmse: 58372.54993|  0:03:01s\n",
      "epoch 399| loss: 961010560.0| train_rmse: 36025.98053| valid_rmse: 44076.50302|  0:03:01s\n",
      "epoch 400| loss: 910794624.0| train_rmse: 29118.19128| valid_rmse: 40861.23966|  0:03:02s\n",
      "epoch 401| loss: 858891264.0| train_rmse: 45309.64367| valid_rmse: 53566.28741|  0:03:02s\n",
      "epoch 402| loss: 1002363904.0| train_rmse: 57743.28582| valid_rmse: 64699.76838|  0:03:03s\n",
      "epoch 403| loss: 1428165632.0| train_rmse: 70292.81863| valid_rmse: 78057.94835|  0:03:03s\n",
      "epoch 404| loss: 1499195136.0| train_rmse: 75981.249| valid_rmse: 84477.03969|  0:03:04s\n",
      "\n",
      "Early stopping occured at epoch 404 with best_epoch = 364 and best_valid_rmse = 38676.17659\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "\n",
    "reg_tabnet = TabNetRegressor(\n",
    "    n_d=20,\n",
    "    n_a=20,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=0.18),\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "\n",
    "reg_tabnet.fit(\n",
    "    X_train=X_train, y_train=y_train.values.reshape(-1,1),\n",
    "    eval_set=[(X_train, y_train.values.reshape(-1,1)), (X_test, y_test.values.reshape(-1,1))],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['rmse'],\n",
    "    max_epochs=1000 , \n",
    "    patience=40,\n",
    "    #batch_size=1024, virtual_batch_size=128,\n",
    "    #num_workers=0,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38676.17658841352"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "y_pred_te = reg_tabnet.predict(X_test)\n",
    "sqrt(mean_squared_error(y_test, y_pred_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_params = {\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'depth': 3,\n",
    "    'eval_metric': 'RMSE',\n",
    "    'fold_len_multiplier': 1.4301509382549529,\n",
    "    'grow_policy': 'Depthwise',\n",
    "    'l2_leaf_reg': 0.0032859943038929806,\n",
    "    'leaf_estimation_backtracking': 'AnyImprovement',\n",
    "    'learning_rate': 0.24999323560121908,\n",
    "    'max_bin': 10,\n",
    "    #'max_leaves': 17,\n",
    "    'min_data_in_leaf': 21,\n",
    "    'random_strength': 0.2417876297895158\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 66434.5627851\ttest: 71265.8045578\tbest: 71265.8045578 (0)\ttotal: 159ms\tremaining: 2m 38s\n",
      "1:\tlearn: 58352.8717430\ttest: 63320.5250020\tbest: 63320.5250020 (1)\ttotal: 163ms\tremaining: 1m 21s\n",
      "2:\tlearn: 52765.6756905\ttest: 57529.0917964\tbest: 57529.0917964 (2)\ttotal: 167ms\tremaining: 55.5s\n",
      "3:\tlearn: 47769.3427375\ttest: 52347.6590845\tbest: 52347.6590845 (3)\ttotal: 170ms\tremaining: 42.3s\n",
      "4:\tlearn: 44382.7045445\ttest: 48413.6067838\tbest: 48413.6067838 (4)\ttotal: 172ms\tremaining: 34.3s\n",
      "5:\tlearn: 41481.6759834\ttest: 45705.0868017\tbest: 45705.0868017 (5)\ttotal: 175ms\tremaining: 29s\n",
      "6:\tlearn: 39622.5273551\ttest: 43904.7916330\tbest: 43904.7916330 (6)\ttotal: 178ms\tremaining: 25.2s\n",
      "7:\tlearn: 37993.6620384\ttest: 42400.3928719\tbest: 42400.3928719 (7)\ttotal: 180ms\tremaining: 22.4s\n",
      "8:\tlearn: 36884.9297062\ttest: 41203.2854774\tbest: 41203.2854774 (8)\ttotal: 183ms\tremaining: 20.2s\n",
      "9:\tlearn: 36202.4997931\ttest: 40415.3997831\tbest: 40415.3997831 (9)\ttotal: 185ms\tremaining: 18.3s\n",
      "10:\tlearn: 35197.7659568\ttest: 39402.5983748\tbest: 39402.5983748 (10)\ttotal: 186ms\tremaining: 16.8s\n",
      "11:\tlearn: 34099.5380757\ttest: 38887.4993448\tbest: 38887.4993448 (11)\ttotal: 188ms\tremaining: 15.5s\n",
      "12:\tlearn: 33453.8682964\ttest: 38602.3547789\tbest: 38602.3547789 (12)\ttotal: 189ms\tremaining: 14.3s\n",
      "13:\tlearn: 32878.6055456\ttest: 38157.6290877\tbest: 38157.6290877 (13)\ttotal: 190ms\tremaining: 13.4s\n",
      "14:\tlearn: 32541.3447570\ttest: 37881.9421824\tbest: 37881.9421824 (14)\ttotal: 191ms\tremaining: 12.6s\n",
      "15:\tlearn: 32359.9897287\ttest: 37714.3224925\tbest: 37714.3224925 (15)\ttotal: 192ms\tremaining: 11.8s\n",
      "16:\tlearn: 32158.4701171\ttest: 37628.1087948\tbest: 37628.1087948 (16)\ttotal: 194ms\tremaining: 11.2s\n",
      "17:\tlearn: 31762.0780496\ttest: 37500.7258019\tbest: 37500.7258019 (17)\ttotal: 195ms\tremaining: 10.6s\n",
      "18:\tlearn: 31548.3277373\ttest: 37296.8637583\tbest: 37296.8637583 (18)\ttotal: 196ms\tremaining: 10.1s\n",
      "19:\tlearn: 31311.0080973\ttest: 37113.7055168\tbest: 37113.7055168 (19)\ttotal: 197ms\tremaining: 9.65s\n",
      "20:\tlearn: 31278.9455278\ttest: 37151.4968065\tbest: 37113.7055168 (19)\ttotal: 198ms\tremaining: 9.24s\n",
      "21:\tlearn: 30974.2400157\ttest: 37063.8143824\tbest: 37063.8143824 (21)\ttotal: 199ms\tremaining: 8.86s\n",
      "22:\tlearn: 30862.9715706\ttest: 36944.5122698\tbest: 36944.5122698 (22)\ttotal: 200ms\tremaining: 8.52s\n",
      "23:\tlearn: 30529.1259899\ttest: 36912.7971901\tbest: 36912.7971901 (23)\ttotal: 202ms\tremaining: 8.2s\n",
      "24:\tlearn: 30311.2977504\ttest: 36570.9444006\tbest: 36570.9444006 (24)\ttotal: 203ms\tremaining: 7.91s\n",
      "25:\tlearn: 29959.2653925\ttest: 36713.7584536\tbest: 36570.9444006 (24)\ttotal: 204ms\tremaining: 7.64s\n",
      "26:\tlearn: 29780.2983124\ttest: 36680.3732518\tbest: 36570.9444006 (24)\ttotal: 205ms\tremaining: 7.39s\n",
      "27:\tlearn: 29621.8549955\ttest: 36586.6463306\tbest: 36570.9444006 (24)\ttotal: 206ms\tremaining: 7.16s\n",
      "28:\tlearn: 29450.2665399\ttest: 36536.7120090\tbest: 36536.7120090 (28)\ttotal: 207ms\tremaining: 6.95s\n",
      "29:\tlearn: 29385.2663493\ttest: 36645.8176943\tbest: 36536.7120090 (28)\ttotal: 208ms\tremaining: 6.74s\n",
      "30:\tlearn: 29326.9588918\ttest: 36715.3163147\tbest: 36536.7120090 (28)\ttotal: 209ms\tremaining: 6.54s\n",
      "31:\tlearn: 29216.2161578\ttest: 36686.5668289\tbest: 36536.7120090 (28)\ttotal: 210ms\tremaining: 6.37s\n",
      "32:\tlearn: 29186.3170027\ttest: 36651.9124851\tbest: 36536.7120090 (28)\ttotal: 211ms\tremaining: 6.19s\n",
      "33:\tlearn: 29108.8655746\ttest: 36558.6811695\tbest: 36536.7120090 (28)\ttotal: 212ms\tremaining: 6.03s\n",
      "34:\tlearn: 28968.7250843\ttest: 36555.2926708\tbest: 36536.7120090 (28)\ttotal: 213ms\tremaining: 5.88s\n",
      "35:\tlearn: 28884.6489188\ttest: 36498.9984024\tbest: 36498.9984024 (35)\ttotal: 214ms\tremaining: 5.74s\n",
      "36:\tlearn: 28816.7374284\ttest: 36507.7125577\tbest: 36498.9984024 (35)\ttotal: 215ms\tremaining: 5.6s\n",
      "37:\tlearn: 28682.9534294\ttest: 36542.7780080\tbest: 36498.9984024 (35)\ttotal: 216ms\tremaining: 5.47s\n",
      "38:\tlearn: 28482.4290702\ttest: 36551.5897300\tbest: 36498.9984024 (35)\ttotal: 217ms\tremaining: 5.35s\n",
      "39:\tlearn: 28441.3788048\ttest: 36589.3194409\tbest: 36498.9984024 (35)\ttotal: 218ms\tremaining: 5.23s\n",
      "40:\tlearn: 28439.2223830\ttest: 36588.0824359\tbest: 36498.9984024 (35)\ttotal: 219ms\tremaining: 5.12s\n",
      "41:\tlearn: 28366.5414086\ttest: 36555.4849416\tbest: 36498.9984024 (35)\ttotal: 220ms\tremaining: 5.02s\n",
      "42:\tlearn: 28221.1395512\ttest: 36591.5291563\tbest: 36498.9984024 (35)\ttotal: 221ms\tremaining: 4.92s\n",
      "43:\tlearn: 28008.3529928\ttest: 36620.1570170\tbest: 36498.9984024 (35)\ttotal: 222ms\tremaining: 4.83s\n",
      "44:\tlearn: 27971.8667990\ttest: 36647.7440910\tbest: 36498.9984024 (35)\ttotal: 223ms\tremaining: 4.73s\n",
      "45:\tlearn: 27879.3245166\ttest: 36711.7258885\tbest: 36498.9984024 (35)\ttotal: 224ms\tremaining: 4.64s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 36498.9984\n",
      "bestIteration = 35\n",
      "\n",
      "Shrink model to first 36 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x21a25d52850>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_cb = cb.CatBoostRegressor(**cb_params, early_stopping_rounds=10)\n",
    "reg_cb.fit(X_train, y_train, eval_set = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36498.998402355035"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "y_pred_te = reg_cb.predict(X_test)\n",
    "sqrt(mean_squared_error(y_test, y_pred_te))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
